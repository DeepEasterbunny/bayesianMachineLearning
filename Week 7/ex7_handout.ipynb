{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as snb\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from jax import value_and_grad\n",
    "from jax import hessian\n",
    "from jax import random\n",
    "\n",
    "# for plotting\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# for manipulating images\n",
    "from PIL import Image\n",
    "\n",
    "from exercise7 import PCA_dim_reduction\n",
    "from exercise7 import visualize_utility\n",
    "from exercise7 import add_colorbar\n",
    "\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# style stuff\n",
    "snb.set_theme(font_scale=1.25)\n",
    "snb.set_style('darkgrid')\n",
    "colors = ['r', 'g', 'b', 'y']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02477 Bayesian Machine Learning - Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "The topic of this exercise is **multi-class classification** and **decision theory**. Probability theory and Bayes' rule tell us how to summarize our knowledge about a parameter or a prediction using probability distributions.  However, often we have to reduce these distributions a single decision, e.g. does the patient have cancer or not, and decision theory tells us how to do that in a principled manner. We will see how the **posterior predictive probabilities** play a key role in making **optimal decisions** and see how the choice of **utility function** affects the resulting decisions. We will also look at how to quantify and represent the predictive uncertainty for multi-class classification and how to investigate the reliability of the posterior predictive probabilities.\n",
    "\n",
    "Studying **Bayesian linear models** for multi-class classification problems are important for many reasons: 1) a well-designed linear model can be hard to beat if data is not abundant and 2) linear models help us understand the theory and build intuition for more complex models. Finally, 3) the last layer of most deep neural networks for classification is equivalent to a linear model with a categorical likelihood. Hence, when we learn model parameters for the last layer only in a transfer learning setting, we actually fit a linear model for classification.\n",
    "\n",
    "- Part 1: Bayesian linear models for multi-class classification\n",
    "- Part 2: Bayesian decision theory for classification\n",
    "- Part 3: Image classification\n",
    "- Part 4: Making decisions with a reject option\n",
    "- Part 5: Model calibration\n",
    "\n",
    "\n",
    "**Note**: The exercise contains several **discussion questions**, which are questions, where are supposed to actively experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs. Instead of proving the full description for every discussion question, we simply tag it with: [**Discussion question**] after the question.\n",
    "\n",
    "\n",
    "Note that the in this exercise, we will need the python packages called **PIL** (for manipulating images) and **autograd** for computing derivatives. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Bayesian linear models for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**From binary to multi-class classification**\n",
    "\n",
    "In week 3, we discussed binary classification using logistic regression models. We used a *Bernoulli likelihood* with the *sigmoid inverse link function* to model the data, i.e. $y_n \\sim \\text{Ber}(\\sigma(f(\\mathbf{x}_n)))$ such that $p(y=1|\\mathbf{x}, \\mathbf{w}) =  \\sigma(f(\\mathbf{x}_n))$. Due to the binary nature of $y_n$, we used that $p(y=0|\\mathbf{x}, \\mathbf{w}) = 1 - p(y=1|\\mathbf{x}, \\mathbf{w}) = 1 - \\sigma(f(\\mathbf{x}_n))$, and hence, the function $f(\\mathbf{x}_n)$ completely specifies the probabilities for both outcomes of $y_n$. \n",
    "\n",
    "This week we will work with the natural extension for multi-class classification. Consider a multi-class problem with $K$ classes and let $\\mathcal{D} = \\left\\lbrace (\\mathbf{x}_i, y_i) \\right\\rbrace_{i=1}^N$ denote a dataset, where $\\mathbf{x}_i \\in \\mathbb{R}^M$ and $y_n \\in \\left\\lbrace 1, 2, \\dots, K\\right\\rbrace$ are the input feature and target label, respectively, for the $i$'th example. \n",
    "\n",
    "Since the targets are **discrete** and **unordered**, the categorical distribution with the softmax function is a standard choice:\n",
    "\n",
    "$$\\begin{align*}\n",
    "y_n|\\mathbf{f}_n &\\sim \\text{Categorical}\\left[\\text{softmax}(\\mathbf{f}_n)\\right], \\tag{1} \n",
    "\\end{align*}$$\n",
    "\n",
    "where ${\\mathbf{f}_n} \\in \\mathbb{R}^K$ is $K$-dimensional latent vector. In contrast to binary classification, $\\mathbf{f}_n = f(\\mathbf{x}_n) \\in \\mathbb{R}^K$ is now a $K$-dimensional vector. The output of the softmax-function is also a vector, where the elements are given by\n",
    "$$\\begin{align*}\n",
    "\\text{softmax}(\\mathbf{f}_n)_i = \\frac{\\exp(\\mathbf{f}_{n,i})}{\\sum_{j=1}^K \\exp( \\mathbf{f}_{n,j})},\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\mathbf{f}_{n,i}$ denotes the $i$'th element of vector $\\mathbf{f}_n$ and the probabilities of the categorical distribution is given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{Categorical}(y_n = i|\\mathbf{f}_n) = \\text{softmax}(\\mathbf{f}_n)_i.\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear models for multi-class classification**\n",
    "\n",
    "For linear models for multi-class classification, we model each entry in $\\mathbf{f}_{n}$ with a separate linear model, i.e.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{f}_{n, i} &= f_i(\\mathbf{x}_n) = \\mathbf{w}_i^T \\phi(\\mathbf{x}_n) \\quad\\text{for}\\quad i = 1, \\dots, K, \\tag{2} \n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\mathbf{w}_i \\in \\mathbb{R}^D$ and $\\phi(\\cdot)$ is a feature expansion such that $\\phi(\\mathbf{x}_n) \\in \\mathbb{R}^D$. \n",
    "Instead of $\\mathbf{f}_{n, i} = \\mathbf{w}_i^T \\mathbf{x}_n $, we could also have used Gaussian processes or neural networks if desired.\n",
    "\n",
    "Note that the linear models in eq. (2) is equivalent to\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{f}_n = \\mathbf{W}\\phi(\\mathbf{x}_n),\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\mathbf{W} \\in \\mathbb{R}^{K \\times D}$ and $\\mathbf{w}_i \\in \\mathbb{R}^D$ is the $i$'th row of $\\mathbf{W}$. This set-up is equivalent to the last layer of a neural network for multi-class classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The joint distribution**\n",
    "\n",
    "To complete the Bayesian model, we impose a prior distribution for each $\\mathbf{w}_i$. Specially, we impose zero-mean Gaussian prior on each $\\mathbf{w}_i$ yielding the following joint distribution:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mathbf{y}, \\mathbf{W}) = \\prod_{n=1}^N p(y_n|\\mathbf{W}) \\prod_{i=1}^K p(\\mathbf{w}_i) = \\prod_{n=1}^N \\text{Cat}(y_n|\\text{Softmax}(\\mathbf{W}\\phi(\\mathbf{x}_n))) \\prod_{i=1}^K \\mathcal{N}(\\mathbf{w}_i|\\mathbf{0}, \\alpha^{-1}\\mathbf{I}).\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Approximate inference using the Laplace approximation**\n",
    "\n",
    "We will again resort to the Laplace approximation for inference because the posterior distribution is analytically intractable (why?) and use **Monte Carlo** sampling to estimate the posterior predictive probabilities $p(y^* = k|\\mathbf{y}, \\mathbf{x}^*)$.\n",
    "\n",
    "Compared to the Gaussian process set-up from last week, this implementation is fairly straight forward. However, to faciliate the implementation of the Laplace approximation, we will let $\\mathbf{w}_{\\text{flat}} = \\begin{bmatrix} \\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_K \\end{bmatrix} \\in \\mathbb{R}^{K \\cdot D}$ be a *flattened* version of $\\mathbf{W} \\in \\mathbf{R}^{K\\times D}$, where $K\\cdot D$ is the total number of parameters of the model. We can then define the Laplace approximation to be\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\mathbf{w}_{\\text{flat}}|\\mathbf{y}) \\approx q(\\mathbf{w}_{\\text{flat}}) = \\mathcal{N}(\\mathbf{w}_{\\text{flat}}|\\mathbf{m}, \\mathbf{S}), \n",
    "\\end{align*}$$\n",
    "where $\\mathbf{m} \\in \\mathbb{R}^{K\\cdot D}$ and $\\mathbf{S} \\in \\mathbb{R}^{K\\cdot D \\times K \\cdot D}$. From these quantities, we can extract the marginal posterior mean and covariance for each $\\mathbf{w}_i$. For exampe, the approximate marginal posterior for $\\mathbf{w}_1$ is $p(\\mathbf{w}_1|\\mathbf{y}) \\approx \\mathcal{N}(\\mathbf{w}_1|\\mathbf{m}_1, \\mathbf{S}_1)$, where the mean $\\mathbf{m}_1 \\in \\mathbb{R}^D$ for $\\mathbf{w}_1$ is given by first $D$ entries in $\\mathbf{m}$ and the marginal posterior covariance $\\mathbf{S}_1 \\in \\mathbb{R}^{D \\times D}$ is the first $D \\times D$-entries in $\\mathbf{S}$ and so on and so forth. But note that $\\mathbf{S}$ also contains information about the posterior covariance between each pair of weight vectors $\\mathbf{w}_i$ and $\\mathbf{w}_j$.\n",
    "\n",
    "In this exercise, we will rely on **JaX** for computing the gradient and Hessian.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making predictions**\n",
    "\n",
    "To make predictions, we approximate the *posterior predictive distribution* using *Monte Carlo estimation*. \n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y^* = k|\\mathbf{y}, \\mathbf{x}^*) \\approx \\int p(y^* =k| \\mathbf{W}) q(\\mathbf{W})\\text{d}\\mathbf{W} \\approx \\frac{1}{S} \\sum_{j=1}^S p(y^* =k| \\mathbf{W}^{(j)}) \\quad\\text{for}\\quad \\mathbf{W}^{(j)} \\sim q(\\mathbf{W})\n",
    "\\end{align*}$$\n",
    "\n",
    " for $j = 1, \\dots, S$. To implement this, we simply draw the samples from $\\mathbf{w}_{\\text{flat}}^{(j)} \\sim \\mathcal{N}(\\mathbf{w}_{\\text{flat}}|\\mathbf{m}, \\mathbf{S})$ and then reshape each sample into $\\mathbf{W}^{(j)} \\in \\mathbb{R}^{K\\times D}$. Note that there also exists an extension of the probit approximation for multi-class classification.\n",
    "\n",
    "\n",
    "**Note:** In the code we will represent the class labels as integers from $0, 1, \\dots, K-1$ rather than from $1, \\dots, K$ because Python counts from 0.\n",
    "\n",
    "We will start with a small toy example with $K = 4$ classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "key = random.PRNGKey(123)\n",
    "keys = random.split(key, 4)\n",
    "\n",
    "# number of data points per class\n",
    "M = 50\n",
    "\n",
    "# generate simple synthetic toy dataset\n",
    "xi = [-3 + random.normal(keys[0], shape=(M, 1)),\n",
    "      -1 + random.normal(keys[1], shape=(M, 1)),\n",
    "       1 + random.normal(keys[2], shape=(M, 1)),\n",
    "       3 + random.normal(keys[3], shape=(M, 1))]\n",
    "x = jnp.concatenate(xi)\n",
    "y = jnp.hstack((jnp.zeros(M), jnp.ones(M), 2*jnp.ones(M), 3*jnp.ones(M)))\n",
    "num_classes = 4\n",
    "\n",
    "# specify input points for predictions\n",
    "xstar = jnp.linspace(-6, 6, 300)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 4))\n",
    "for i in range(4):\n",
    "    ax.hist(xi[i], density=True, label='Class %d' % i, alpha=0.75, color=colors[i]);\n",
    "ax.set(xlabel='Input feature', title='Class-conditional distributions')\n",
    "ax.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**\n",
    "\n",
    "**Task 1.1**: Complete the implementation of the `log_prior` and `log_likelihood` functions below.\n",
    "\n",
    "*Hints:**\n",
    "- *All the parameters of the model are i.i.d. as $\\mathcal{N}(0, \\alpha^{-1})$ under the prior distribution.*\n",
    "- *The class variable `y_onehot` contains a onehot-representations of $\\mathbf{y}$. Since we rely on JaX and autograd to compute the gradients and Hessian, you can only use `jax.numpy` functions in your implementation.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2**: Complete the implementation of `predict_y` below\n",
    "\n",
    "*Hints:*\n",
    "-  *Generate the desired number of samples from $\\mathbf{w}^{(j)}_{\\text{flat}} \\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{S})$ and then reshape each sample to $K \\times D$ weight matrix, e.g. $\\mathbf{W}^{(j)} \\in \\mathbb{R}^{K \\times D}$, and then perform the Monte Carlo estimation.*\n",
    "- *If you want to check your implementation of the previous task before attempting this one, you can comment out the lines `phat = ...` and `axes[2].plot(xstar, phat...`. If you do so, you should see a plot of the posterior mean of the $K$ linear models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_npdf(x, m, v):\n",
    "    return -0.5*(x-m)**2/v - 0.5*jnp.log(2*jnp.pi*v)\n",
    "\n",
    "# convert from class label to one-hot encoding\n",
    "def to_onehot(y, num_classes):\n",
    "    return jnp.column_stack([1.0*(y==value) for value in jnp.arange(num_classes)])\n",
    "\n",
    "\n",
    "\n",
    "class BayesianLinearSoftmax(object):\n",
    "    \"\"\" Bayesian linear softmax classifier with i.i.d. Gaussian priors \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, alpha=1):\n",
    "        \n",
    "        # data and prior\n",
    "        self.X, self.y  = X, y\n",
    "        self.N, self.D = self.X.shape\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # num classes, parameters and one-hot encoding\n",
    "        self.num_classes = len(jnp.unique(y))\n",
    "        self.num_params = self.num_classes * self.D\n",
    "        self.y_onehot = to_onehot(self.y, self.num_classes)\n",
    "        \n",
    "        # fit\n",
    "        self.compute_laplace_approximation()\n",
    "\n",
    "\n",
    "    def log_prior(self, w_flat):\n",
    "        \"\"\" Evaluates the log prior, i.e. log p(W). \n",
    "            The function accepts the argument w_flat, which is a flattened version of W, such that the shape of w_flat is (T,), where T = num_classes x D is the total number of parameters.\n",
    "            The return value of the function must be a scalar.\n",
    "        \"\"\"\n",
    "        log_prior_val = <insert code here>\n",
    "\n",
    "        # check dimensions and return\n",
    "        assert log_prior_val.shape == ()\n",
    "        return log_prior_val\n",
    "    \n",
    "    \n",
    "        \n",
    "    def log_likelihood(self, w_flat):\n",
    "        \"\"\" Evaluates the log likelihood for dataset (self.X, self.y) using a Categorical distribution with softmax inverse link function\n",
    "            The function accepts the argument w_flat, which is a flattened version of W, such that the shape of w_flat is (T,), where T = num_classes x D is the total number of parameters.\n",
    "            The return value of the function must be a scalar.\n",
    "        \"\"\"\n",
    "        \n",
    "        # reshape from flat vector to matrix of size num_classes by D\n",
    "        W = w_flat.reshape((self.num_classes, self.D))\n",
    "\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "\n",
    "        # check dimensions and return\n",
    "        assert loglik_val.shape == ()\n",
    "        return loglik_val\n",
    "        \n",
    "    def log_joint(self, w_flat):\n",
    "        return self.log_prior(w_flat) + self.log_likelihood(w_flat)\n",
    "    \n",
    "    def compute_laplace_approximation(self):\n",
    "        \"\"\" computes Laplace approximation of model \"\"\"\n",
    "\n",
    "        w_init_flat = jnp.zeros(self.num_params)\n",
    "        cost_fun = lambda W: -self.log_joint(W)\n",
    "        result = minimize(value_and_grad(cost_fun), w_init_flat, jac=True)\n",
    "\n",
    "        if result.success:\n",
    "            w_MAP = result.x\n",
    "            self.m_flat = w_MAP[:, None]    \n",
    "            self.A_flat = hessian(cost_fun)(w_MAP)\n",
    "            self.S_flat = jnp.linalg.inv(self.A_flat)\n",
    "            return self.m_flat, self.S_flat\n",
    "        else:\n",
    "            print('Warning optimization failed')\n",
    "            return None, None\n",
    "    \n",
    "    def predict_f(self, X_star):\n",
    "        \"\"\" computes the posterior distribution of f_i(x, w) = w_i^T phi(x) for all classes \"\"\"\n",
    "        \n",
    "        # get relevant part for each of the K functions\n",
    "        m = self.m_flat.reshape((self.num_classes, self.D))\n",
    "        Si = [self.S_flat[i*self.D:(i+1)*self.D, i*self.D:(i+1)*self.D] for i in range(self.num_classes)]\n",
    "    \n",
    "        # compute mean and variance for each function\n",
    "        mu_f_all_classes = X_star@m.T\n",
    "        var_f_all_classes = jnp.squeeze(jnp.stack([jnp.diag(X_star@Si[i]@X_star.T) for i in range(self.num_classes)], axis=1))\n",
    "\n",
    "        return mu_f_all_classes, var_f_all_classes\n",
    "        \n",
    "    \n",
    "    def predict_y(self, X_star, num_samples=500, seed=123):\n",
    "        \"\"\" computes and returns p(y^*=k|y, x^*) using Monte Carlo sampling\n",
    "         \n",
    "            Arguments:\n",
    "            X_star            --         PxD prediction points\n",
    "            num_samples       --         number of Monte Carlo samples to use\n",
    "            seed              --         seed for random number generator\n",
    "\n",
    "            Returns\n",
    "            p_all             --         Post. pred. probabilities for each point in X_star for each class, must be PxK numpy array, where K is the number of classes\n",
    "        \"\"\"\n",
    "\n",
    "        key = random.PRNGKey(seed)\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "        \n",
    "        assert p_all.shape == (len(X_star), self.num_classes), f\"The shape of p_all was expected to be ({len(X_star)}, {self.num_classes}), but the actual shape was {p_all.shape}. Please check the code\"\n",
    "        return p_all\n",
    "    \n",
    "\n",
    "# linear model with intercept and slope\n",
    "design_matrix = lambda x: jnp.column_stack((jnp.ones(len(x)), x))\n",
    "\n",
    "# fit model and compute predictions\n",
    "model = BayesianLinearSoftmax(design_matrix(x), y)\n",
    "mu_f, var_f = model.predict_f(design_matrix(xstar))\n",
    "phat = model.predict_y(design_matrix(xstar))\n",
    "\n",
    "# plot\n",
    "fig, axes = plt.subplots(1,3, figsize=(20, 5))\n",
    "for i in range(4):\n",
    "    # plot histogram of data\n",
    "    axes[0].hist(xi[i], density=True, label='Class %d' % i, alpha=0.75, color=colors[i]);\n",
    "    # plot posterior mean of latent function y for each class\n",
    "    axes[1].plot(xstar, mu_f[:, i], label='i = %d' % i, color=colors[i])    \n",
    "    # plot posterior class probabilitites\n",
    "    axes[2].plot(xstar, phat[:, i], label='i = %d' % i, color=colors[i])\n",
    "    \n",
    "axes[0].legend()\n",
    "for i in range(3):\n",
    "    axes[i].set_xlabel('Input x')\n",
    "    \n",
    "axes[0].set_title('Data')\n",
    "axes[1].set_title('Posterior mean for $f^*_k$')\n",
    "axes[2].set_title('Posterior class probabilities $p(y^*=k|, \\\\mathbf{y}, x^*)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3**: Study the plots above. Explain the role of the softmax-function in eq. (1) above and use the figures above to explain why the name \"softmax\" makes sense [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4**: Implement and plot the **confidence** and **entropy** for the predictive distributions above for each value of $x$ in the vector `xstar` (similar to the rightmost plot above). Comment on the relation between the confidence and entropy plots and the posterior predictive probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Bayesian decision theory for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a decision for multi-class classification entails assigning a class label $\\hat{y}^* \\in \\left\\lbrace 1, 2, \\dots, K\\right\\rbrace$ to a new test point $\\mathbf{x}^*$. In Bayesian decision theory, the utility function $\\mathcal{U}(y^*, \\hat{y})$ specifies the **utility** (i.e. gain) for predicting $\\hat{y}^*$ when the true target is $y^*$.\n",
    "\n",
    "In practice, we don't know the true target $y^*$, but the predictive posterior distribution $p(y^*|\\mathbf{y}, \\mathbf{x}^*)$ contains all the relevant knowledge about $y^*$ given our observed data $\\mathbf{y}$. Therefore, we compute the **expected utility** wrt. the posterior predictive distribution for each possible value of $\\hat{y}$ and then assign the class label that **maximizes the expected utility**. That is,\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y}^* = \\arg\\max\\limits_{k \\in \\left\\lbrace 1, 2, \\dots, K\\right\\rbrace} \\mathbb{E}_{p(y^*|\\mathbf{y}, \\mathbf{x}^*)}\\left[\\mathcal{U}(y^*, \\hat{y}=k)\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Since $p(y^*|\\mathbf{y}, \\mathbf{x}^*)$ is a discrete probability distribution with probabilities $p(y^* = k|\\mathbf{y}, \\mathbf{x}^*) = \\pi_k$, expectations with respect to $p(y^*|\\mathbf{y}, \\mathbf{x}^*)$ is simply a weighted sum.\n",
    "\n",
    "In the videos we saw that in order to make optimal decisions under the **0/1-utility function** (remember utility is just negative loss and vice versa), we simply have to pick the class label with largest posterior probability. Let's investigate this empirically and study how the decision regions change when we change the utility-function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.1**: Complete the implementation of the function `compute_expected_utility`\n",
    "\n",
    "*Hints: How do we compute expectations with respect to discrete distributions?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_expected_utility(U, phat):\n",
    "    \"\"\" computes the expected utility for a multi-class classification problem with K classes for utility matrix U and posterior predictive probabilities phat \n",
    "        \n",
    "        Arguments\n",
    "        U               --      Utility matrix (shape: [K x K])\n",
    "        phat            --      Posterior predictive probabilities (shape: [P x K]), where P is the number of prediction points\n",
    "\n",
    "        expected_util   --      Expected utility for each class for each point in phat (shape: P x K)           \n",
    "           \"\"\"\n",
    "    \n",
    "\n",
    "    expected_util = <insert code here>\n",
    "\n",
    "    # check dimensions and return\n",
    "    assert expected_util.shape == phat.shape, f'The variable expected_util was expected to have shape {phat.shape}, but the actual shape was {expected_util.shape}. Please check your code.'\n",
    "    return expected_util\n",
    "\n",
    "# define utility matrix\n",
    "U = jnp.identity(num_classes)\n",
    "\n",
    "\n",
    "\n",
    "# compute the expected utility for each class\n",
    "expected_utility = compute_expected_utility(U, phat) \n",
    "\n",
    "# make decisions\n",
    "decisions = to_onehot(jnp.argmax(expected_utility, axis=1), num_classes)\n",
    "    \n",
    "# plot everything\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 9))\n",
    "\n",
    "# utility matrix\n",
    "visualize_utility(axes[0,0], U)\n",
    "\n",
    "# posterior predictive probs\n",
    "for i in range(num_classes):\n",
    "    axes[0,1].plot(xstar, phat[:, i], color=colors[i])\n",
    "axes[0,1].set_title('Posterior predictive probabilities', fontweight='bold')\n",
    "axes[0,1].set_xlabel('Input x')\n",
    "\n",
    "# expected utility\n",
    "for i in range(num_classes):\n",
    "    axes[1,0].plot(xstar, expected_utility[:, i], color=colors[i]);\n",
    "axes[1,0].set_title('Expected utility for each class', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Input x')\n",
    "\n",
    "# decisions\n",
    "for i in range(num_classes):\n",
    "    axes[1,1].plot(xstar, decisions[:, i], color=colors[i], linewidth=2);\n",
    "    axes[1,1].plot(xstar, phat[:, i], color=colors[i], alpha=0.5, linestyle='--')\n",
    "axes[1,1].set_title('Decision regions', fontweight='bold')\n",
    "axes[1,1].set_xlabel('Input x')\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2**: Use the figure above to explain how Bayesian decision theory works for multi-class classification. [**Discussion question**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3**: What happens to the expected utilities and to the decisions if you scale the utility matrix by a positive constant? [**Discussion question**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.4**: What happens to the decision boundary if you introduce a negative utility of $-1$ for predicting 1 (green), when the true target is 0 (red)? What about $-2$? [**Discussion question**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5**: What happens if you change the 0/1 utility function to have $U_{23} = U_{32} = 1$? [**Discussion question**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.6**: What happens if you change the 0/1 utility function to have $U_{11} = 0$? or to $U_{11} = 0.5$? [**Discussion question**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to apply the material from part 1 and part 2 to a real dataset. Specifically, we will work with a subset of the Linnaeus 5 dataset (http://chaladze.com/l5/). The original Linnaeus 5 dataset contains images of size 256x256 from 5 classes, but we will work with a subset of this dataset containing a total of 3200 images in 4 classes (dogs, birds, flowers, berries). The images have been resized to 128x128 with the sole purpose of reducing the size of the data file. \n",
    "\n",
    "We will use **transfer learning** and use a pretrained ResNet18-network as a **feature extractor** for the images. ResNet18 is a convolutional neural network with 18 layers, which has been trained on a huge image dataset called ImageNet. The ImageNet containes images from 1000 different classes, which means that the very last layer of the ResNet architecture is a softmax-layer with 1000 outputs. However, if we get rid of the very last layer, we can use the rest of the network as a general feature extractor for images. That is, we propagate each image through the network we can use the very last hidden layer as a 512-dimensional feature vector for the image.\n",
    "\n",
    "The details of how this works beyond what's written above is **not** part of the curriculum of the course and therefore, we have pre-computed feature vectors for all the images for you. \n",
    "\n",
    "However, if you are interested in the details, you can look at the following resources:\n",
    "\n",
    "- Code used for feature extraction [here](https://github.com/christiansafka/img2vec)\n",
    "- Paper describing the ResNet architecture [here](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "- A Pytorch tutorial for transfer learning for vision problems [here](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#sphx-glr-beginner-transfer-learning-tutorial-py)\n",
    "\n",
    "\n",
    "Let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = jnp.load('./ex7_data.npz')\n",
    "labels = list(data['labels'])\n",
    "targets = data['targets']\n",
    "num_classes = data['num_classes'][()]\n",
    "\n",
    "img_train, img_test = data['Xtrain'], data['Xtest']\n",
    "ytrain, ytest = data['ttrain'], data['ttest']\n",
    "train_idx = data['train_idx']\n",
    "test_idx = data['test_idx']\n",
    "\n",
    "N, D = img_train.shape\n",
    "Ntest = len(img_test)\n",
    "print(f'Number of images for training: {N}')\n",
    "print(f'Number of images for test: {Ntest}')\n",
    "print(f'Number of features: {D}')\n",
    "print(f'Number of clases: {num_classes}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and plot a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_example(ax, i):\n",
    "    \"\"\" show training example i \"\"\"\n",
    "    \n",
    "    j = train_idx[i]\n",
    "    img = Image.open('./images/%d.jpg' % j)\n",
    "    target = targets[j]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(labels[int(target)])\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "\n",
    "fig, ax = plt.subplots(2, 8, figsize=(20, 6))\n",
    "for i in range(16):\n",
    "    show_example(ax.flat[i], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is represented using a 512-dimensional feature vector, but we will (again) reduce the dimensionality to $D = 2$ using principal component analysis (PCA) for the purpose of visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Ztrain, Ztest = PCA_dim_reduction(img_train, img_test, num_components=2)\n",
    "\n",
    "def plot_pca_data(ax, legend=True, alpha=1):\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        ax.plot(Ztest[ytest==i, 0], Ztest[ytest==i, 1], '.', color='k', markersize=14, alpha=0.4)\n",
    "        ax.plot(Ztest[ytest==i, 0], Ztest[ytest==i, 1], '.', color=colors[i], label=labels[i], markersize=6, alpha=alpha)\n",
    "    if legend:\n",
    "        ax.legend(markerscale=2)\n",
    "    ax.set(xlabel='PC1', ylabel='PC2')\n",
    "    ax.set_title('Test data')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_pca_data(ax);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's  fit the model and visualize the posterior class probabilitites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_regions(x_grid, posterior_class_probs, name, show_data=True):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 6))\n",
    "    for i in range(num_classes):\n",
    "        \n",
    "        probs_reshaped =  posterior_class_probs[:, i].reshape((len(x_grid), len(x_grid)))\n",
    "        im = axes[i].pcolormesh(x_grid, x_grid,probs_reshaped, cmap=plt.cm.RdBu_r, clim=(0, 1), shading='auto')\n",
    "\n",
    "        if show_data:\n",
    "            plot_pca_data(axes[i], legend=False)\n",
    "        axes[i].set_title('Class %d: %s' % (i, labels[i]))\n",
    "\n",
    "        if i > 0:\n",
    "            axes[i].set_yticklabels([])\n",
    "            axes[i].set_ylabel('')\n",
    "\n",
    "    fig.subplots_adjust(right=0.9, wspace=0.01)\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.025, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax);\n",
    "    fig.suptitle(name, fontweight='bold', y=1.025)\n",
    "        \n",
    "# we want to have an intercept in the model\n",
    "X_train, X_test = design_matrix(Ztrain), design_matrix(Ztest)\n",
    " \n",
    "# prepare grid for making predictions\n",
    "x_grid = jnp.linspace(-3.5, 3.5, 100)\n",
    "XX1, XX2 = jnp.meshgrid(x_grid, x_grid)\n",
    "Xp = jnp.column_stack((XX1.ravel(), XX2.ravel()))\n",
    "X_pred = design_matrix(Xp)\n",
    "\n",
    "# Fit linear classifier\n",
    "model = BayesianLinearSoftmax(X_train, ytrain)\n",
    "p_pred = model.predict_y(X_pred)\n",
    "\n",
    "# visualize the posterior class proabilities for each model\n",
    "visualize_regions(x_grid, p_pred, 'Posterior predictive probabilities for Bayesian Linear Softmax model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1**: Compute predictions (wrt. the 0/1-utility function) and compute the accuracy for the training and test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.2**: Compute and plot the **entropy** and **confidence** for the predictions in the plot above. In which areas of the input space is this model most uncertain about the class label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3**: Compute the average confidence for the training set and test set and accuracy results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4:  Making decisions with a reject option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will investigate how to make decisions with a **reject** option, meaning we avoid to making any decisions if the confidence is below a specified threshold $p_{\\text{reject}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify reject threshold\n",
    "p_reject = 0.6\n",
    "\n",
    "# compute decisions under 0/1-utility function and reshape to grid\n",
    "decisions = jnp.argmax(p_pred, axis=1).reshape((len(x_grid), len(x_grid)))\n",
    "\n",
    "# identify reject regions\n",
    "p_pred_confidence = confidence(p_pred) # use the function you implemented earlier\n",
    "reject_region_bls = 1.0*jnp.logical_not((p_pred_confidence < p_reject).reshape((len(x_grid), len(x_grid))))\n",
    "decisions_with_reject = (decisions+1)*reject_region_bls\n",
    "\n",
    "# visualize\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "plot_pca_data(axes[0])\n",
    "axes[0].pcolormesh(x_grid, x_grid, decisions, cmap=ListedColormap(colors), alpha=1, shading='auto')\n",
    "axes[0].set_title('Decision regions for Bayesian Linear softmax')\n",
    "plot_pca_data(axes[1]);\n",
    "axes[1].pcolormesh(x_grid, x_grid, decisions_with_reject, cmap=ListedColormap(['k'] + colors), shading='auto')\n",
    "axes[1].set_title('Decision regions w. reject for Bayesian Linear Softmax');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.1**: Explain what you see in the figure above - relate the **reject region** in black to the confidence plot above [**Discussion question**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task 4.2**: What happens to the reject region if you increase or decrease the reject threshold?  [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.3**: How many percent of the samples in the test set are rejected with $p_{\\text{reject}} = 0.6$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.4**: What is the test accuracy rate for the samples in the test set, which are not rejected?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.5**: If we were to make a decision for the test samples in the reject region, what would the accuracy rate be? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 4.6**: Instead, if the reject option, implement the following utility function and plot the corresponding decision boundaries:\n",
    "\n",
    "- Utility of 1 for correctly classifying dogs, birds, and flowers\n",
    "- Utility of 2 for correctly classifying berries\n",
    "- Utility of 0 for all misclassifications\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5:  Model calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, the posterior predictive probabilities play a key role in decision-making. In the last part of this exercise, we will study calibration of these probabilities using the **expected calibration error (ECE)**-metric and so-called **reliability plots** (see Section 14.2.2.1 or pages 578 in Murphy2).\n",
    "\n",
    "In this exercise, we will use the test set $\\left\\lbrace \\mathbf{x}_m^*, y^*_m \\right\\rbrace_{m=1}^M$ to evaluate these metrics. Let $\\hat{y}^*_m = \\arg\\max_k p(y^*_m = k|\\mathbf{y}, \\mathbf{x}^*m)$ be the decision (under the 0/1-utility function) for the $m$'th test point and let $\\mathcal{C}_m = \\max_k p(y^*_m = k|\\mathbf{y}, \\mathbf{x}^*_m)$ be the corresponding confidence. We will divide the unit interval in $B$ bins and estimate the average accuracy and confidence for each bin:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{acc}(\\mathcal{B}_b) &= \\frac{1}{|\\mathcal{B}_b|} \\sum_{m \\in \\mathcal{B}_b} \\mathbb{I}\\left(\\hat{y}^*_m = y^*_m\\right)\\\\\n",
    "\\text{conf}(\\mathcal{B}_b) &= \\frac{1}{|\\mathcal{B}_b|} \\sum_{m \\in \\mathcal{B}_b} \\mathcal{C}_m,\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\mathcal{B}_b$ contains all the indices of points falling into the $b$'th bin. Then the ECE is defined as\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{ECE} = \\sum_{b=1}^B \\frac{|\\mathcal{B}_b|}{M}|\\text{acc}(\\mathcal{B}_b) - \\text{conf}(\\mathcal{B}_b)|.\n",
    "\\end{align*}$$\n",
    "\n",
    "**Note**: The ECE is **not** a so-called *proper scoring rule*, and therefore, we rely solely ECE for model evaluation. In fact, a model can be useless without any predictive power, but still be perfectly calibrated. Therefore, we should always also look at metrics like accuracy, ELPD, log likelihood etc. to make sure the model performs as intended. However, the ECE can be really useful to assess the calibration-aspect specifically.\n",
    "\n",
    "It is often also useful to plot $\\text{acc}(\\mathcal{B}_b)$ as a function of $\\text{conf}(\\mathcal{B}_b)$ for each bin. This is called **reliability plot** or **calibration plot** and may provide insights into whether the model are generally over or underconfident. \n",
    "\n",
    "\n",
    "**Task 5.1**: Set $\\alpha = 1$. Compute the posterior predictive distribution for the test set of the image classification problem. Compute and plot a histogram of the confidences for each predictions. Repeat for $\\alpha=10^{-2}$ and $\\alpha=10^2$. How does $\\alpha$ generally affect the confidences? Can you explain why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 5.2**: Implement a function for estimating the expected calibration error (ECE) metric as well as for making a reliability plot. Compute the ECE and the plot reliability for the following values of $\\alpha \\in \\left\\lbrace 10^{-2}, 1, 10^2 \\right\\rbrace$. Compare the reliability plots to the histograms in the previous exercise.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02477",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
