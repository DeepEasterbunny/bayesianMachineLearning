{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3efd343a",
   "metadata": {},
   "source": [
    "## Beta binomial model (Week 1)\n",
    "Modeling of a chance of an event (beta part) and it occuring N times (binomial), e.g. clicks on a webpage. Say we showed 123 people and add, and 8 clicked on it, we would like to know the effectiveness of the add\n",
    "\n",
    "### Model:\n",
    "$$ \\begin{align*}\n",
    "    p(\\theta) &= \\text{Beta}(\\theta|a_0, b_0)&&\\text{(Prior)}\\\\\n",
    "    p(y|\\theta) &= {N \\choose y} \\theta^y (1-\\theta)^{N-y}&&\\text{(Likelihood)}\\\\\n",
    "    p(\\theta|y) &= \\text{Beta}(\\theta|a_0 + y, b_0 + N-y)&&\\text{(Posterior)}\n",
    "  \\end{align*} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472764b",
   "metadata": {},
   "source": [
    "## Simple logistic regression, with two parameters (Week 2)\n",
    "Goal: Predciting number of fractured O-rings. We needed to compute probability of one O-ring fracturing, and extending this\n",
    "\n",
    "\n",
    "$$\\begin{align*}\n",
    "y_i|\\theta_i \\sim \\text{Bin}(N_i, \\theta_i).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\begin{align}\n",
    "\\theta(x) = \\sigma(\\alpha + \\beta x), \\nonumber\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\\begin{align}\n",
    "f(x) = \\alpha + \\beta x.\\nonumber\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "p(y_i|x_i, \\alpha, \\beta) = \\text{Bin}(y_i|N_i, \\theta_i), \\nonumber\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\\begin{align}\n",
    "p(\\mathbf{y}|\\mathbf{x}, \\alpha, \\beta) = \\prod_{i=1}^M p(y_i|x_i, \\alpha, \\beta) = \\prod_{i=1}^M  \\text{Bin}(y_i|N, \\theta_i),\\nonumber\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\\begin{align}\n",
    "p(y^*|x^*, \\alpha, \\beta) = \\text{Bin}(y^*|N^*, \\theta^*),\\nonumber\n",
    "\\end{align}\n",
    "$ \n",
    "\n",
    "Prior\n",
    "$\\begin{align}\n",
    "p(\\alpha, \\beta) = \\mathcal{N}(\\alpha|0, \\sigma^2_\\alpha)\\mathcal{N}(\\beta|0, \\sigma^2_{\\beta})\\nonumber\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "Joint distribution\n",
    "$\\begin{align}\n",
    "p(\\mathbf{y}, y^*, \\alpha, \\beta|\\mathbf{x}, x^*) = p(y^*|x^*, \\alpha, \\beta)p(\\mathbf{y}|\\mathbf{x}, \\alpha, \\beta) p(\\alpha, \\beta) = \\underbrace{\\text{Bin}(y^*|N^*, \\theta^*)}_{p(y^*|x^*, \\alpha, \\beta)}\\underbrace{\\prod_{i=1}^M  \\text{Bin}(y_i|N, \\theta_i)}_{p(\\mathbf{y}|\\mathbf{x}, \\alpha, \\beta)} \\underbrace{\\mathcal{N}(\\alpha|0, \\sigma^2_\\alpha)\\mathcal{N}(\\beta|0, \\sigma^2_{\\beta})}_{p(\\alpha, \\beta)},\\nonumber\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e283f3c3",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b620b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "sigmoid = lambda x: 1./(1 + jnp.exp(-x))\n",
    "log_npdf = lambda x, m, v: -(x-m)**2/(2*v) - 0.5*jnp.log(2*jnp.pi*v)\n",
    "# Logistic regression \n",
    "class LogisticRegression(object):\n",
    "\n",
    "    def __init__(self, x, y, N, sigma2_alpha=1., sigma2_beta=1.):\n",
    "        # data\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.N = N\n",
    "\n",
    "        # hyperparameters\n",
    "        self.sigma2_alpha = sigma2_alpha\n",
    "        self.sigma2_beta = sigma2_beta\n",
    "\n",
    "    def f(self, x, alpha, beta):\n",
    "        \"\"\" implements eq. (3). Output must have the same shape as x \"\"\"\n",
    "        return alpha + x * beta\n",
    "        \n",
    "    def theta(self, x, alpha, beta):\n",
    "        \"\"\" implements eq. (2). Output must have the same shape as x \"\"\"\n",
    "        return sigmoid(self.f(x, alpha, beta))\n",
    "\n",
    "    def log_prior(self, alpha, beta):\n",
    "        \"\"\" implements log. of eq. (8). Output must have the same shape as alpha and beta \"\"\"\n",
    "        return log_npdf(alpha, 0 , self.sigma2_alpha) + log_npdf(beta, 0, self.sigma2_beta)\n",
    "\n",
    "    def log_likelihood(self, alpha, beta):\n",
    "        \"\"\" implements log. of eq. (5). Output must have the same shape as alpha and beta \"\"\"\n",
    "        theta = self.theta(self.x, alpha, beta)\n",
    "        loglik = binom.logpmf(k = self.y, n = self.N, p = theta)\n",
    "        s = jnp.sum(loglik, axis = -1, keepdims=True)\n",
    "        return s\n",
    "\n",
    "    def log_joint(self, alpha, beta):\n",
    "        return self.log_prior(alpha, beta).squeeze() + self.log_likelihood(alpha, beta).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0516eb7",
   "metadata": {},
   "source": [
    "## Linear Regression (Week 3)\n",
    "\n",
    "## Model\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_n = f(\\mathbf{x}_n) + e_n = \\phi(\\mathbf{x}_n)^T \\mathbf{w} + e_n = \\mathbf{\\phi}_n^T \\mathbf{w} + e_n,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Joint distribution\n",
    "$$ \\begin{align*}\n",
    "p(\\mathbf{y}, \\mathbf{w}) = p\\left(\\mathbf{y}|\\mathbf{w}\\right)p\\left(\\mathbf{w}\\right) = \\mathcal{N}\\left(\\mathbf{y}\\big|\\mathbf{\\Phi}\\mathbf{w}, \\sigma^2\\mathbf{I}\\right)\\mathcal{N}\\left(\\mathbf{w}\\big|\\mathbf{m}_0, \\mathbf{S}_0\\right),\n",
    "\\end{align*} $$\n",
    "\n",
    "**posterior predictive distribution**\n",
    "$$\\begin{align*}\n",
    "p(y^*|\\mathbf{y}, \\mathbf{x}^*) &= \\int p(y^*|\\mathbf{x}^*, \\mathbf{w})p(\\mathbf{w}|\\mathbf{y})\\text{d}\\mathbf{w} = \\int \\mathcal{N}(y^*|\\phi^T_n \\mathbf{w}, \\beta^{-1}) \\mathcal{N}(\\mathbf{w}|\\mathbf{m}, \\mathbf{S}) \\text{d}\\mathbf{w}\n",
    "= \\mathcal{N}(y_*|\\phi_*^T\\mathbf{m}, \\phi_*^T\\mathbf{S} \\phi_* + \\beta^{-1}),\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf04cd",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9900a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinearRegression(object):\n",
    "    \n",
    "    def __init__(self, Phi, y, alpha=1., beta=1.):\n",
    "        \n",
    "        # store data and hyperparameters\n",
    "        self.Phi, self.y = Phi, y\n",
    "        self.N, self.D = Phi.shape\n",
    "        self.alpha, self.beta = alpha, beta\n",
    "        \n",
    "        # compute posterior distribution\n",
    "        self.m, self.S = self.compute_posterior(alpha, beta)\n",
    "        self.log_marginal_likelihood = self.compute_marginal_likelihood(alpha, beta)\n",
    "\n",
    "        # perform sanity check of shapes/dimensions\n",
    "        self.check_dimensions()\n",
    "\n",
    "    def check_dimensions(self):\n",
    "        D = self.D\n",
    "        assert self.y.shape == (self.N, 1), f\"Wrong shape for data vector y.\\n For N = {N}, the shape of y must be ({N}, 1), but the actual shape is {self.y.shape}\"\n",
    "        assert self.m.shape == (D, 1), f\"Wrong shape for posterior mean.\\nFor D = {D}, the shape of the posterior mean must be ({D}, 1), but the actual shape is {self.m.shape}\"\n",
    "        assert self.S.shape == (D, D), f\"Wrong shape for posterior covariance.\\nFor D = {D}, the shape of the posterior mean must be ({D}, {D}), , but the actual shape is {self.S.shape}\"\n",
    "\n",
    "    def compute_posterior(self, alpha, beta):\n",
    "        \"\"\" computes the posterior N(w|m, S) and return m, S.\n",
    "            Shape of m and S must be (D, 1) and (D, D), respectively  \"\"\"\n",
    "        \n",
    "        #############################################\n",
    "        # Insert your solution here\n",
    "        #############################################\n",
    "        S = jnp.linalg.inv((alpha * jnp.identity(n = self.D) + beta * self.Phi.T @ self.Phi))\n",
    "        m = beta * S @ self.Phi.T @ self.y\n",
    "        #############################################\n",
    "        # End of solution\n",
    "        #############################################\n",
    "        return m, S\n",
    "      \n",
    "    def generate_prior_samples(self, key, num_samples):\n",
    "        \"\"\" generate samples from the prior  \"\"\"\n",
    "        return random.multivariate_normal(key, jnp.zeros(len(self.m)), (1/self.alpha)*jnp.identity(len(self.m)), shape=(num_samples, ))\n",
    "    \n",
    "    def generate_posterior_samples(self, key, num_samples):\n",
    "        \"\"\" generate samples from the posterior  \"\"\"\n",
    "        return random.multivariate_normal(key, self.m.ravel(), self.S, shape=(num_samples, ))\n",
    "    \n",
    "    def predict_f(self, Phi):\n",
    "        \"\"\" computes posterior mean (mu_f) and variance (var_f) of f(phi(x)) for each row in Phi-matrix.\n",
    "            If Phi is a [N, D]-matrix, then the shapes of both mu_f and var_f must be (N,)\n",
    "            The function returns (mu_f, var_f)\n",
    "        \"\"\"\n",
    "\n",
    "        mu_f = (self.m.T @ Phi.T).flatten()\n",
    "        var_f = jnp.diag((Phi @ self.S @ Phi.T))\n",
    "        # check dimensions before returning values\n",
    "        assert mu_f.shape == (Phi.shape[0],), \"Shape of mu_f seems wrong. Check your implementation\"\n",
    "        assert var_f.shape == (Phi.shape[0],), \"Shape of var_f seems wrong. Check your implementation\"\n",
    "        return mu_f, var_f\n",
    "        \n",
    "    def predict_y(self, Phi):\n",
    "        \"\"\" returns posterior predictive mean (mu_y) and variance (var_y) of y = f(phi(x)) + e for each row in Phi-matrix.\n",
    "            If Phi is a [N, D]-matrix, then the shapes of both mu_y and var_y must be (N,).\n",
    "            The function returns (mu_y, var_y)\n",
    "        \"\"\"\n",
    "        mu_f, var_f = self.predict_f(Phi)\n",
    "        mu_y = mu_f \n",
    "        var_y = var_f + 1/self.beta \n",
    "        # check dimensions before returning values\n",
    "        assert mu_y.shape == (Phi.shape[0],), \"Shape of mu_y seems wrong. Check your implementation\"\n",
    "        assert var_y.shape == (Phi.shape[0],), \"Shape of var_y seems wrong. Check your implementation\"\n",
    "        return mu_y, var_y\n",
    "        \n",
    "    \n",
    "    def compute_marginal_likelihood(self, alpha, beta):\n",
    "        \"\"\" computes and returns log marginal likelihood p(y|alpha, beta) \"\"\"\n",
    "        inv_S0 = alpha*jnp.identity(self.D)\n",
    "        A = inv_S0 + beta*(self.Phi.T@self.Phi)\n",
    "        m = beta*jnp.linalg.solve(A, self.Phi.T)@self.y   # (eq. 3.53 in Bishop)\n",
    "        S = jnp.linalg.inv(A)                             # (eq. 3.54 in Bishop)\n",
    "        Em = beta/2*jnp.sum((self.y - self.Phi@m)**2) + alpha/2*jnp.sum(m**2)\n",
    "        return self.D/2*jnp.log(alpha) + self.N/2*jnp.log(beta) - Em - 0.5*jnp.linalg.slogdet(A)[1] - self.N/2*jnp.log(2*jnp.pi)\n",
    "         \n",
    "\n",
    "    def optimize_hyperparameters(self):\n",
    "        # optimizes hyperparameters using marginal likelihood\n",
    "        theta0 = jnp.array((jnp.log(self.alpha), jnp.log(self.beta)))\n",
    "        def negative_marginal_likelihood(theta):\n",
    "            alpha, beta = jnp.exp(theta[0]), jnp.exp(theta[1])\n",
    "            return -self.compute_marginal_likelihood(alpha, beta)\n",
    "\n",
    "        result = minimize(value_and_grad(negative_marginal_likelihood), theta0, jac=True)\n",
    "\n",
    "        # store new hyperparameters and recompute posterior\n",
    "        theta_opt = result.x\n",
    "        self.alpha, self.beta = jnp.exp(theta_opt[0]), jnp.exp(theta_opt[1])\n",
    "        self.m, self.S = self.compute_posterior(self.alpha, self.beta)\n",
    "        self.log_marginal_likelihood = self.compute_marginal_likelihood(self.alpha, self.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2675297d",
   "metadata": {},
   "source": [
    "## Whe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
