{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02477 Bayesian Machine Learning - Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import jax.numpy as jnp\n",
    "import seaborn as snb\n",
    "\n",
    "from scipy.stats import binom as binom_dist\n",
    "from scipy.stats import beta as beta_dist\n",
    "from scipy.special import beta as beta_fun\n",
    "\n",
    "snb.set_style('darkgrid')\n",
    "snb.set(font_scale=1.5)\n",
    "plt.rcParams['lines.linewidth'] = 3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to become familiar with the core components of Bayesian inference: the **prior**, the **likelihood**, **posterior** and **the model evidence**. We will also re-cap various ways to summarize distributions, e.g. **mean**, **mode**, **variance**, and **intervals**, and we will look into how to compute and manipulate probabilities using sampling. We will study these concepts in the context of the Beta-Binomial model, which is the \"Hello world\"-equivalent of Bayesian statistics.\n",
    "\n",
    "One of the main applications of the Beta-Binomial model is to **estimate proportions**. For example, suppose a website shows a specific ad to $N = 112$ customers and $y = 13$ of those costumers end up clicking on the ad. A common task is then to estimate the click-rate for this ad in order to answer questions like the following:\n",
    "\n",
    "1) What is the probability that the next customer will click on the ad?\n",
    "\n",
    "2) What is the probability that the click-rate is below 10%?\n",
    "\n",
    "We will see how the Bayesian Beta-Binomial model can be used to answer such questions. Furthermore, we will conclude the exercise by studying a slightly more general version of the problem: Suppose a website has two ads: version A and version B and that version A was shown $N_A$ times and generated $y_A$ clicks, whereas version B was shown $N_B$ times and generated $y_B$ click. What is the probability that the click-rate of version B is larger than click-rate of version A?\n",
    "\n",
    "We highly encourage you to **discuss your results and conclusions with one of the teachers/teaching assistant to check you understanding**. Moreover, if you get stuck with an exercise, don't hesitate to consult the solution or the teachers/teaching assistant.\n",
    "\n",
    "**Content**\n",
    "\n",
    "- Part 1: Maximum likelihood estimation\n",
    "- Part 2: Bayesian inference\n",
    "- Part 3: The functional form of Beta distributions\n",
    "- Part 4: Computing summary statistics and probabilities using sampling\n",
    "- Part 5: Application to A/B testing\n",
    "\n",
    "\n",
    "**Note on JaX**\n",
    "\n",
    "In this course, we will be using the [JaX-framework](https://github.com/jax-ml/jax) for numerical computations. The module `jax.numpy` implements an interface very similar to the familiar `numpy` for basic operations, but JaX is much faster than numpy and JaX also features GPU support, automatic differentiation, just-in-time compilations, and many other advanced features. However, we won't be using the advanced features of JaX in this course. The biggest difference between `jnp.numpy` and `numpy` is how random number generation is handled, but we will get into that later.\n",
    "\n",
    "We typically use\n",
    "\n",
    "`import jax.numpy as jnp`\n",
    "\n",
    "to remind ourselves that we are working JaX and not regular numpy. JaX can be installed using most environment managers, e.g. pip:\n",
    "\n",
    "`pip install -U jax`\n",
    "\n",
    "For more information about JaX, consult the website: [https://github.com/jax-ml/jax](https://github.com/jax-ml/jax).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the proportions using the binomial distribution\n",
    "\n",
    "In this exercise, we will work with two common families of probabilities distributions: the **Binomial distribution** and the **Beta distribution**. \n",
    "\n",
    "### The Binomial distribution and maximum likelihood estimation\n",
    "\n",
    "First, we re-cap the ***Binomial distribution***. The Binomial distribution is a discrete probability distribution representing the number of successes in a sequence of **conditionally independent Bernoulli trials** and it is specified through its **probability mass function (PMF)**:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(y|N, \\theta) &= \\text{Bin}(y|N, \\theta)\\\\\n",
    "&= {N\\choose y} \\theta^{y}(1-\\theta)^{N-y},\n",
    "\\end{align*}$$\n",
    "\n",
    "where $N$ is the number of trials, $\\theta \\in \\left[0, 1\\right]$ is the probability of success in each individual trial and $y \\in \\left\\lbrace 0, 1, \\dots, N \\right\\rbrace$ is the total number of successes. The quantity ${N\\choose y}$ is a called a binomial coefficient and is pronounced \"$N$ choose $y$\" and counts the number of ways $y$ items can be chosen from a set of $N$ items. The simplest way to estimate $\\theta$ is through **maximum likelihood estimation (MLE)**, which for this model can be done analytically:\n",
    "\n",
    "$$\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_{\\theta} p(y|N, \\theta) = \\frac{y}{N}.$$\n",
    "\n",
    "We can also compute a classic/frequentist 95%-**confidence interval**:\n",
    "\n",
    "$$\\hat{\\theta}_{\\text{MLE}} \\pm 1.96 \\sqrt{\\frac{\\hat{\\theta}_{\\text{MLE}}(1-\\hat{\\theta}_{\\text{MLE}})}{N}}.$$\n",
    "\n",
    "Technically, this type of interval is called a **Wald interval** and relies on a Gaussian approximation, but we won't cover these details in this course. Note that a **confidence interval** is **generally not** the same as a **posterior credibility interval** (see Section 4.6.6 in Murphy1). \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The beta distribution as prior distribution for the proportion $\\theta$\n",
    "\n",
    "The ***Beta distribution*** is distribution over random variables in the unit interval. The **probability density function (PDF)** of the Beta distribution has two parameters $a_0 > 0$ and $b_0 > 0$ and is given by\n",
    "\n",
    "$$p(\\theta|a_0,b_0) = \\frac{1}{B(a_0,b_0)}\\theta^{a_0-1}(1-\\theta)^{b_0-1},$$\n",
    "\n",
    "where $B(a_0, b_0)$ is a **normalization constant** that ensures that the density integrates to one:\n",
    "\n",
    "$$B(a_0,b_0) = \\int \\theta^{a_0-1}(1-\\theta)^{b_0-1} \\text{d}\\theta = \\frac{\\Gamma(a_0)\\Gamma(b_0)}{\\Gamma(a_0+b_0)},$$\n",
    "\n",
    "where $\\Gamma$ is the so-called gamma function, which we won't dive deeper into in this course. Since $B(a_0,b_0)$ is a constant wrt. $\\theta$, it holds that\n",
    "\n",
    "$$p(\\theta|a_0,b_0) \\propto \\theta^{a_0-1}(1-\\theta)^{b_0-1},$$\n",
    "\n",
    "and therefore, we say that $f(\\theta) = \\theta^{a-1}(1-\\theta)^{b-1}$ is the **functional form** of a Beta distribution for some $a,b > 0$.\n",
    "\n",
    "The **mean** of a Beta-distributed random variable, $\\theta \\sim \\text{Beta}(a_0, b_0)$, is given by\n",
    "\n",
    "$$\\mathbb{E}\\left[\\theta\\right] = \\int_0^1 \\theta \\, p(\\theta|a_0,b_0) \\,\\text{d} \\theta  = \\frac{a_0}{a_0+b_0}. \\tag{1}$$\n",
    "\n",
    "The quantity $\\mathbb{E}\\left[\\theta\\right]$ is also sometimes refered to as the **expected value** of $\\theta$ or the **first moment** of $\\theta$. The **variance** of $\\theta$ is\n",
    "\n",
    "$$\\mathbb{V}\\left[\\theta\\right] = \\int_0^1 \\left(\\theta - \\mathbb{E}\\left[\\theta\\right] \\right)^2 \\, p(\\theta|a_0,b_0) \\,\\text{d} \\theta  = \\frac{a_0 b_0}{(a_0+b_0)^2(a_0+b_0+1)}.$$\n",
    "\n",
    "When $a_0, b_0 > 1$ the Beta density is **unimodal** with the mode (i.e. the location of the peak of the distribution) given by \n",
    "\n",
    "$$\\theta_{\\text{mode}} = \\frac{a_0 - 1}{a_0 + b_0 - 2}.$$\n",
    "\n",
    "If needed, you can find more information about both distributions in Section 2.4.1 (Murphy1) in the textbook or on Wikipedia: \n",
    "\n",
    "[https://en.wikipedia.org/wiki/Beta_distribution](https://en.wikipedia.org/wiki/Beta_distribution)\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Binomial_distribution](https://en.wikipedia.org/wiki/Binomial_distribution)\n",
    "\n",
    "### The Beta-binomial model ###\n",
    "\n",
    "\n",
    "The beta-binomial model is a Bayesian model for estimating proportions $\\theta \\in \\left[0, 1\\right]$, where the **likelihood** is the binomial distribution and a Beta distribution is used as a **prior distribution** for the parameter $\\theta$. The key equations for the model are given by\n",
    "\n",
    "\\begin{align*}\n",
    "    p(\\theta) &= \\text{Beta}(\\theta|a_0, b_0)&&\\text{(Prior)}\\\\\n",
    "    p(y|\\theta) &= {N \\choose y} \\theta^y (1-\\theta)^{N-y}&&\\text{(Likelihood)}\\\\\n",
    "    p(\\theta|y) &= \\text{Beta}(\\theta|a_0 + y, b_0 + N-y)&&\\text{(Posterior)}\n",
    "  \\end{align*}\n",
    "\n",
    "\n",
    "for the dataset $\\mathcal{D} = \\left\\lbrace N, y \\right\\rbrace$. \n",
    "\n",
    "**Notation**\n",
    "\n",
    "Following the convention of the book, we will the use both $p(\\theta|y)$ and $p(\\theta|\\mathcal{D})$ to denote the posterior distribution of $\\theta$ conditioned on $y$. We will often write $p(\\theta)$ to denote the prior instead of $p(\\theta|a_0,b_0)$ and usually only use  the latter if we want to highlight the dependency on the hyperparameters $a_0, b_0$. \n",
    "\n",
    "We will typically refer to $\\theta$ as the **parameter** of the model and to $a_0, b_0$ as **hyperparamters** of the model.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:  Maximum likelihood estimation\n",
    "Assume you want to estimate the probability of success, $\\theta \\in \\left[0, 1\\right]$, after observing $y = 1$ success out of $N = 7$ independent Bernoulli trials. That is, your dataset is given by $\\mathcal{D} = \\left\\lbrace N = 7, y = 1\\right\\rbrace$.\n",
    "\n",
    "**Task 1.1**: Plot the likelihood $p(y|\\theta)$ as a function of $\\theta$ for $\\theta \\in \\left[0,1\\right]$ and identify the maximum likelihood solution visually/numerically. \n",
    "\n",
    "*Hints:*\n",
    "\n",
    "- *You can either implement the likelihood function yourself using the equation given above, or you can use the implementation from scipy.stats: binom_dist.pmf(y, n, p), where y is the number of success, n is the number of trials and p is the probability of success.*\n",
    "- *If you don't know how to get started, study the solution.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "N1 = 7\n",
    "y1 = 1\n",
    "\n",
    "# make grid for plotting the likelihood p(y|theta) in interval [0, 1]\n",
    "thetas = jnp.linspace(0, 1, 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task 1.2**: Compute the maximum likelihood estimate for $\\theta$ and compute a 95% confidence interval using the equations given above.\n",
    "\n",
    "\n",
    "\n",
    "**Task 1.3**: What happens if you had observed $y = 0$ instead of $y = 1$? Does the result seem reasonable?\n",
    "\n",
    "***Note**: *This is a discussion question, which means that you actively have to experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:  Bayesian inference\n",
    "\n",
    "We will now turn our attention towards Bayesian inference for $\\theta$. Recall, the core concept of Bayesian inference is that we infer a **full probability distribution**  for $\\theta$ rather than just a **point estimate** like $\\hat{\\theta}_{MLE}$. \n",
    "As before, your dataset is given by $\\mathcal{D} = \\left\\lbrace N = 7, y = 1\\right\\rbrace$, but now we assume a **uniform prior distribution** for $\\theta$, i.e. $p(\\theta) = \\text{Beta}(\\theta|a_0,b_0) = 1$ for $a_0 = b_0 = 1$.\n",
    "\n",
    "**Task 2.1**: Compute the **prior** mean and variance of $\\theta$, i.e. the mean and variance of $p(\\theta)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task 2.2**: Compute the parameters $a$ and $b$ of the posterior distribution, i.e. $p(\\theta|y)$, using the equations for the Beta-binomial model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3**: Plot the prior density $p(\\theta)$, likelihood $p(y|\\theta)$, and the posterior density $p(\\theta|y)$ as a function of $\\theta$ for $\\theta \\in \\left[0, 1\\right]$ in the same figure.\n",
    "\n",
    "*Hints: the functions beta_dist.pdf and binom_dist.pmf might come in handy*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.4**: Compute the **MAP-estimator** for $\\theta$ as well as the posterior mean of $\\theta$. \n",
    "\n",
    "*Hint*: *The MAP-estimator is the mode of the posterior density, i.e. $\\theta_{\\text{MAP}} = \\arg\\max\\limits_{\\theta \\in \\left[0, 1\\right]} p(\\theta|y)$*, and can be computed analytically for the Beta-binomoial model.\n",
    "\n",
    "\n",
    "\n",
    "**Task 2.5**: Compute a 50%, 90% and a 95% posterior credibility interval for $\\theta$.\n",
    "\n",
    "*Hints*:\n",
    "-  To obtain a 50% posterior credibility interval, our goal is to identify $\\theta_1, \\theta_2 \\in \\left[0, 1\\right]$ such that  $p(\\theta \\in \\left[\\theta_1, \\theta_2\\right]|\\mathcal{D}) = \\int _{\\theta_1}^{\\theta_2} p(\\theta|\\mathcal{D}) \\text{d} \\theta  \\approx 0.5$\n",
    "- *scipy.stats.beta.interval* might come in handy for this.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.6**: What happens if you had observed $y = 0$ instead of $y = 1$? Does the result seem reasonable?\n",
    "\n",
    "**Note**: *This is a discussion question, which means that you actively have to experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.7**: Experiment with different values of $a_0$, $b_0$, $N$, and $y$ to explore how it affects the results (e.g. the plots, MAP, posterior mean and posterior credibility interval).\n",
    "\n",
    "**Note**: *This is a discussion question, which means that you actively have to experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few tasks, we will explore the relationships between the posterior mean and the maximum likelihood estimator.\n",
    "\n",
    "**Task 2.8** Show that the posterior mean converges to the maximum likelihood estimator $\\hat{\\theta}_{\\text{MLE}} = \\frac{y}{N}$ as $N \\rightarrow \\infty$.\n",
    "\n",
    "*Hints:*\n",
    "- *Write the posterior mean as a function of $a_0, b_0, N, y$*\n",
    "- *Write the number of successes as $y = \\hat{\\theta}_{MLE} N$ and substitute it into the expression for the posterior mean*\n",
    "- *If you are stuck, don't hesitate to consult the solution or ask the teachers/teaching assistant for an additional hint*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Task 2.9** Show that the posterior mean is always between the prior mean, $\\theta_0 = \\frac{a_0}{a_0 + b_0}$, and the maximum likelihood estimate $\\hat{\\theta}_{MLE} = \\frac{y}{N}$.\n",
    "\n",
    "Hints:\n",
    "- Show that the posterior mean is a convex combination of the prior mean $\\theta_0$ and the maximum likelihood solution $\\hat{\\theta}_{\\text{MLE}}$, i.e. that the posterior mean can be written as \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}\\left[\\theta|\\mathcal{D}\\right] = (1-\\lambda) \\theta_0 + \\lambda \\hat{\\theta}_{MLE}\n",
    "\\end{align*}\n",
    "$$\n",
    "for some $0 \\leq \\lambda \\leq 1$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3:  The functional form of Beta distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are given the expression for a probability density function $p_d(\\theta)$ up to a constant, i.e. you are told that $p_d(\\theta) =   \\frac{1}{Z_d}\\theta^{36}(1-\\theta)^{41}$, where $Z_d > 0$ is an unknown, but positive constant.\n",
    "\n",
    "\n",
    "**Task 3.1**: Argue the distribution $p_d$ specified above must be a Beta-distribution $p(\\theta|a_d, b_d)$ and identify its parameters $a_d, b_d$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Task 3.2**: Compute $Z_d$\n",
    "\n",
    "*Hint: What is the normalization constant for a Beta distribution?*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use our knowledge of the functional form for Beta densities to compute the denominator in Bayes' theorem, $p(y)$, which is often called the **model evidence** or the **marginal likelihood**. It can be expressed using the **product rule** and the **sum rule** of probability theory:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y) = \\underbrace{\\int p(y, \\theta) \\text{d}\\theta}_{\\text{sum rule}} = \\int \\underbrace{p(y|\\theta)p(\\theta)}_{\\text{product rule}} \\text{d}\\theta .\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Later in the course, we will see that this term can be useful for hyperparameter tuning and model selection. For most models of practical interest, the term will be **intractable** because we cannot solve the integral above analytically.  However, for models like the beta-binomial we actually compute this term in closed-form.\n",
    "\n",
    "**Task 3.3**: Compute the analytical expression for the  model evidence for the Beta-Binomial model\n",
    "\n",
    "**Hints**:\n",
    "- Insert the probability mass function for the binomial likelihood and the probability density function for the beta distribution in the integral given above.\n",
    "- Use linearity of integrals to \"move\" constants (wrt. $\\theta$) outside the integral\n",
    "- Identify the resulting integral as the integral of the functional form corresponding to a Beta density.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4:  Computing summary statistics and probabilities using sampling\n",
    "\n",
    "Once, we have obtained our posterior distribution of interest, we often compute the relevant **summary statistics** using **sampling** when the quantities can not easily be computed analytically. We can often generate a set of samples to represent the distribution and then compute the quantities of interest based on the samples. For example,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters for posterior distribution\n",
    "a = 6\n",
    "b = 17\n",
    "\n",
    "# generate samples\n",
    "num_samples = 100000\n",
    "theta_samples = beta_dist.rvs(a=a, b=b, size=num_samples)\n",
    "\n",
    "# plot\n",
    "thetas = jnp.linspace(0, 1, 200)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n",
    "ax.plot(thetas, beta_dist.pdf(thetas, a=a, b=b), label='$p(\\\\theta|y)$')\n",
    "ax.hist(theta_samples, 50, density=True, label='Histogram of posterior samples of $\\\\theta$', alpha=0.5, color='g')\n",
    "ax.set(xlabel='$\\\\theta$')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the posterior samples $\\theta^{(i)} \\sim p(\\theta|y)$ for $i = 1, \\dots, S$, we can easily **estimate** the posterior mean and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytical_posterior_mean = a/(a+b)\n",
    "analytical_posterior_variance = (a*b)/((a+b)**2*(a+b+1))\n",
    "\n",
    "print(f'E[theta|D] = {jnp.mean(theta_samples):5.4f} (estimated using samples)')\n",
    "print(f'E[theta|D] = {analytical_posterior_mean:5.4f} (analytical solution)\\n')\n",
    "print(f'V[theta|D] = {jnp.var(theta_samples):5.4f} (estimated using samples)')\n",
    "print(f'V[theta|D] = {analytical_posterior_variance:5.4f} (analytical solution)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling is often easy to implement, and hence, it can also be a highly valuable method for verifying analytical results. \n",
    "\n",
    "We can also estimate probabilities and credibility intervals using samples as follows. Suppose we want to estimate the posterior probability that $\\theta > 0.2$, then we generate $S$ samples from the posterior, i.e. $\\theta^{(i)} \\sim p(\\theta|\\mathcal{D})$ for $i = 1, ..., S$, and then simply count the fraction of samples satisfying $\\theta^{(i)} < 0.2$. The reason this works is that we can phrase  the probability as an expectation value, which can be estimated using so-called **Monte Carlo samples**:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\theta > 0.2 | \\mathcal{D}) = \\int_{0.2}^1 p(\\theta|\\mathcal{D}) \\text{d} \\theta = \\int_0^1 \\mathbb{I}\\left[\\theta > 0.2\\right] p(\\theta|\\mathcal{D}) \\text{d}\\theta = \\mathbb{E}_{p(\\theta|\\mathcal{D})}\\left[\\mathbb{I}\\left[\\theta > 0.2\\right]\\right] \\approx \\frac{1}{S}\\sum_{i=1}^S \\mathbb{I}\\left[\\theta^{(i)} > 0.2\\right],\n",
    "\\end{align*}$$\n",
    "where $\\mathbb{I}\\left[\\cdot\\right]$ is the indicator function yielding $1$ if the condition in the brackets are true, and 0 otherwise. We will talk much more about Monte Carlo sampling later in the course, but for now, we will simply use it as tool to summarize distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'P[theta > 0.2|D] = {jnp.mean(theta_samples > 0.2):5.4f}\\t\\t\\t(estimated using sampling)\\n')\n",
    "\n",
    "interval = jnp.percentile(theta_samples, jnp.array([2.5, 97.5]))\n",
    "print(f'95% credibility interval: [{interval[0]:4.3f}, {interval[1]:4.3f}]\\t(estimated using sampling)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the larger number of samples $S$ used, the more accurate an estimate we will get. Later in the course, we will make this statement much more precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "A friend of yours is building a classifier for a company, and she asks for your help to evaluate the model. On an independent test set of $N = 100$ examples, the classifier made $y = 8$ errors.  It is critical for the company that the error rate is below 10%. Your friend argues that the error rate is $\\frac{8}{100} = 0.08$, so there no need to worry, but you are not as convinced because of the rather small test set.\n",
    "\n",
    "Let $\\theta$ represent the error rate and assume a flat Beta-prior, i.e. $a_0 = b_0 = 1$. \n",
    "\n",
    "**Task 4.1**: Compute the posterior mean of the error rate $\\theta$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task 4.2**: Generate $S = 10000$ samples from the posterior distribution and estimate the posterior probability of the test error being larger than $10%$.  Comment on the result.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5:  Application to A/B testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suppose a website has two ads: version A and version B and that version A was shown $N_A = 947$ times and generated $y_A = 87$ clicks, whereas version B was shown $N_B = 1053$ times and generated $y_B = 101$ click.\n",
    "\n",
    "We will now put everything together and apply it do a Bayesian analysis of the data using the Beta-binomial model. \n",
    "\n",
    "**Task 5.1** Assuming a $\\text{Beta}(\\theta|2, 2)$ prior for both $\\theta_A$ and $\\theta_B$, plot the posterior density for both ads.\n",
    "\n",
    "\n",
    "\n",
    "**Task 5.2** Estimate the mean and a 95%-credibility interval for both ads. Use a $p(\\theta) = \\text{Beta}(\\theta|2, 2)$ prior for both ads.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.3** Generate $S = 10000$ posterior samples for both ads and plot the histograms of both sets of samples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.4** Compute posterior samples for the difference of $\\theta_D = \\theta_B - \\theta_A$ and visualize the histogram\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 5.5** Compute the posterior mean and 95% credibility interval for $\\theta_D$ using the posterior samples\n",
    "\n",
    "\n",
    "\n",
    "**Task 5.6** What is the posterior probability that the click-rate of version B is larger than click-rate of version A?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATEL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
