{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02477 Bayesian Machine Learning - Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import seaborn as snb\n",
    "\n",
    "\n",
    "from scipy.stats import binom as binom_dist\n",
    "\n",
    "snb.set_style('darkgrid')\n",
    "snb.set_theme(font_scale=1.)\n",
    "colors = 'brgmc'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The purpose of this exercise is to dive deeper into the basic concepts of probabilistic and Bayesian modelling using **logistic regression** as a case-study. Specifically, we will study:\n",
    "\n",
    "- the probability distributions involved in logistic regression\n",
    "- how to approximate the posterior distribution of non-conjugate models using simple grid approximations\n",
    "- how to make predictions using posterior predictive distributions\n",
    "\n",
    "We will study these concepts using data from Challenger Space Shuttle disaster.\n",
    "\n",
    "\n",
    "**Content**\n",
    "\n",
    "- Part 1: Setting up the logistic regression model\n",
    "- Part 2: The prior, likelihood and posterior\n",
    "- Part 3: Making predictions using point estimates\n",
    "- Part 4: Approximating the posterior using a grid approximation\n",
    "- Part 5: Propagating uncertainty from parameters to predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: The Challenger Space Shuttle disaster \n",
    "\n",
    "On a cold morning on January 28, 1986 the space shuttle called **Challenger** tragically exploded 73 seconds after its launch, killing all 7 crew members. The explosion was later determined to be caused the failure of two out of six so-called o-ring seals in one of the rocket boosters.\n",
    "The temperature on the day of the launch was close $0^{\\circ} C$, which was record-low. The cold weather made the o-rings stiffen, reducing their ability to seal the critical joints, and hence, causing the tragic accident. O-ring failure had previously been observed on 6 of out the 23 prior launches as plotted below.\n",
    "\n",
    "In this exercise, you will construct a simple Bayesian logistic regression model for predicting the probability of o-ring failure as a function of temperature.\n",
    "\n",
    "The dataset $\\mathcal{D} = \\left\\lbrace (t_i, y_i) \\right\\rbrace_{i=1}^{M}$ consists of $M=23$ observations, where $y_i \\in \\left[0, 1, \\dots 6\\right]$ denotes the number of failed o-rings for the $i$'th temperature $t_i$. The number of o-rings for all launches were $N = 6$, and hence, the fraction of failed o-rings for the $i$'th launch is $\\frac{y_i}{N} = \\frac{y_i}{6}$.\n",
    "\n",
    "See [Wikipedia](https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster) for more background information.\n",
    "\n",
    "\n",
    "First, we will load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv file uisng pandas\n",
    "data = pd.read_csv('./o-ring.csv')\n",
    "t = data['temperature'].values\n",
    "y = data['failed'].values\n",
    "N = data['count'].values\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for plotting data\n",
    "def plot_data(ax=None, counts=True):\n",
    "    if counts:\n",
    "        ax.plot(t, y, 'mo', label='Data: $y_i$')\n",
    "        ax.set(ylabel='Number of failed o-rings')\n",
    "    else:\n",
    "        ax.plot(t, y/N, 'ko', label='Data: $y_i/N$')\n",
    "        ax.set(ylabel='Fraction of failed o-rings')\n",
    "    ax.set(xlabel='Temperature (celcius)')\n",
    "    ax.legend(loc='lower left')\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 4))\n",
    "plot_data(ax[0])\n",
    "plot_data(ax[1], counts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than working directly with the temperatures $t_i \\in \\mathbb{R}$, we will standardize the temperatures to get\n",
    "\n",
    "$$\\begin{align*}\n",
    "x_i = \\frac{t_i - t_{\\text{mean}}}{t_{\\text{std}}},\n",
    "\\end{align*}$$\n",
    "where $t_{\\text{mean}}$ and $t_{\\text{std}}$ are the empirical mean and standard deviation, respectively, of the observed temperatures $\\left\\lbrace t_i \\right\\rbrace_{i=1}^M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean and standard deviation of temperatures\n",
    "tmean, tstd = jnp.mean(t), jnp.std(t)\n",
    "\n",
    "# standardize temperatures\n",
    "standardize = lambda t_: (t_ - tmean)/tstd\n",
    "x = standardize(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:  Setting up the logistic regression model\n",
    "\n",
    "### The big picture\n",
    "\n",
    "Our goal is to use the training data, i.e. the pairs of $\\mathbf{x}_i$ and $y_i$, to learn a model such that we can make predictions and reason about the number of failures $y^*$ for a new input point $x^*$. In probabilistic terms, our goal is to compute $p(y^*|\\mathbf{y}, \\mathbf{x}, x^*)$. That is, the distribution of the number of failures $y^*$ **conditioned on** the observed data $\\mathbf{x}, \\mathbf{y}$ as well as the new input point $x^*$. In probabilistic machine learning, we generally refer to $p(y^*|\\mathbf{y}, \\mathbf{x}, x^*)$ as the **posterior predictive distribution**.\n",
    "Once we computed this distribution, we can use it to reason about $y^*$ and answer questions like:\n",
    "- What is the expected number of failures?\n",
    "- What is the most likely number of failures?\n",
    "- What is the probability that the number of failures is zero?\n",
    "- What is a 90% interval credibility interval for the number of failures?\n",
    "\n",
    "Before we can compute $p(y^*|\\mathbf{y}, \\mathbf{x}, x^*)$, we first need to specify a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the model\n",
    "\n",
    "For each temperature $x_i$, we know the number of failed o-ring $y_i = \\{0, 1, \\dots, 6 \\}$ out of the total number of o-rings $N_i = 6$. Hence, the number of failing o-rings $y_i$ for a given temperature $x_i$ can be described by a **Binomial distribution** with $N_i = 6$ and probability $\\theta_i \\in \\left[0, 1\\right]$: \n",
    "\n",
    "$$\\begin{align}\n",
    "y_i|\\theta_i \\sim \\text{Bin}(N_i, \\theta_i).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is equivalent to modelling the failure of each individual o-ring as **conditionally independent** Bernoullli trials. Since all observations have $N_i = 6$, we drop the subscript $i$ and simply write $N = 6$ in the following to ease the notation.\n",
    "\n",
    "Rather than imposing Beta-distributions as prior distributions for each $\\theta_i$, we assume $\\theta_i$ can be modelled as is a function of the temperature. Specifically, we assume\n",
    "\n",
    "$\\begin{align}\n",
    "\\theta(x) = \\sigma(\\alpha + \\beta x),\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where $\\alpha, \\beta \\in \\mathbb{R}$ are parameters to be estimated and $\\sigma: \\mathbb{R} \\rightarrow \\left[0, 1\\right]$ is the **logistic sigmoid function**, i.e. $\\sigma(x) = (1+\\exp(-x))^{-1}$. For later references, we will denote the linear component of the model as $f(x)$, i.e.\n",
    "\n",
    "$\\begin{align}\n",
    "f(x) = \\alpha + \\beta x.\n",
    "\\end{align}\n",
    "$\n",
    "Note that both $\\theta(x)$ and $f(x)$ do indeed depend on $\\alpha$ and $\\beta$, but we suppress this dependency to ease the notation. The likelihood of the $i$'th data point thus becomes\n",
    "$\n",
    "\\begin{align}\n",
    "p(y_i|x_i, \\alpha, \\beta) = \\text{Bin}(y_i|N_i, \\theta_i),\n",
    "\\end{align}\n",
    "$\n",
    "where we have defined $\\theta_i \\equiv \\theta(x_i) = \\sigma(\\alpha+\\beta x_i)$. Assuming that all observations $\\left\\lbrace y_i \\right\\rbrace_{i=1}^M$ are **conditionally independent** given $\\theta_i$, the likelihood of the full dataset becomes\n",
    "\n",
    "$\\begin{align}\n",
    "p(\\mathbf{y}|\\mathbf{x}, \\alpha, \\beta) = \\prod_{i=1}^M p(y_i|x_i, \\alpha, \\beta) = \\prod_{i=1}^M  \\text{Bin}(y_i|N, \\theta_i),\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where $\\mathbf{y} = [y_1, y_2, \\dots, y_M]$ and $\\mathbf{x} = [x_1, x_2, \\dots, x_M]$. Our goal is to be able to predict $y^*$ for a new temperature $x^*$.  To do this we write down the *predictive likelihood* for $y^*$, i.e.\n",
    "\n",
    "$\\begin{align}\n",
    "p(y^*|x^*, \\alpha, \\beta) = \\text{Bin}(y^*|N^*, \\theta^*),\n",
    "\\end{align}\n",
    "$ \n",
    "\n",
    "where $\\theta^* \\equiv \\theta(x^*) = \\sigma(\\alpha + \\beta x^*)$. Using the **product rule** and the assumption of conditional independence, we have\n",
    "\n",
    "$\\begin{align}\n",
    "p(\\mathbf{y}, y^*|\\mathbf{x}, x^*, \\alpha, \\beta) = p(\\mathbf{y}|\\mathbf{x}, \\alpha, \\beta)p(y^*|x^*, \\alpha, \\beta) = \\underbrace{\\prod_{i=1}^M  \\text{Bin}(y_i|N, \\theta_i)}_{p(\\mathbf{y}|\\mathbf{x}, \\alpha, \\beta)} \\underbrace{\\text{Bin}(y^*|N^*, \\theta^*)}_{p(y^*|x^*, \\alpha, \\beta)}.\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "To complete the Bayesian model, we need to impose a prior distribution on the parameters $\\alpha, \\beta$. Since we do not have any information about the parameters, we will impose generic Gaussian priors with zero mean:\n",
    "\n",
    "$\\begin{align}\n",
    "p(\\alpha, \\beta) = \\mathcal{N}(\\alpha|0, \\sigma^2_\\alpha)\\mathcal{N}(\\beta|0, \\sigma^2_{\\beta})\n",
    "\\end{align}\n",
    "$\n",
    "for $\\sigma^2_\\alpha, \\sigma^2_\\beta > 0$. This leads to the following **joint distribution** for $y^*, \\mathbf{y}, \\alpha,$ and $\\beta$:\n",
    "\n",
    "$\\begin{align}\n",
    "p(\\mathbf{y}, y^*, \\alpha, \\beta|\\mathbf{x}, x^*) = p(y^*|x^*, \\alpha, \\beta)p(\\mathbf{y}|\\mathbf{x}, \\alpha, \\beta) p(\\alpha, \\beta) = \\underbrace{\\text{Bin}(y^*|N^*, \\theta^*)}_{p(y^*|x^*, \\alpha, \\beta)}\\underbrace{\\prod_{i=1}^M  \\text{Bin}(y_i|N, \\theta_i)}_{p(\\mathbf{y}|\\mathbf{x}, \\alpha, \\beta)} \\underbrace{\\mathcal{N}(\\alpha|0, \\sigma^2_\\alpha)\\mathcal{N}(\\beta|0, \\sigma^2_{\\beta})}_{p(\\alpha, \\beta)},\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where we again used the product rule.\n",
    "\n",
    "The joint distribution is a central object because it completely determines how all variables (e.g. both data and parametes) interact and thus, it completely specifies our probabilistic model. From the joint distribution, we can derive any related distribution of interest using the rules of probability theory. \n",
    "\n",
    "### From the joint distribution to the posterior predictive distribution\n",
    "\n",
    "Because we observe $\\mathbf{y}$, we naturally consider the **conditional distribution** of $y^, \\alpha, \\beta$ given the observed quantities $\\mathbf{y}, \\mathbf{x}, x^*$. Using the standard rules for condtional distributions, we get:\n",
    "\n",
    "$\\begin{align}\n",
    "p(y^*, \\alpha, \\beta|\\mathbf{y}, \\mathbf{x}, x^*) = \\frac{p(\\mathbf{y}, y^*, \\alpha, \\beta|\\mathbf{x}, x^*)}{p(\\mathbf{y}|\\mathbf{x})}.\n",
    "\\end{align}$\n",
    "\n",
    "Our primary goal is to compute the **posterior predictive distribution** $p(y^*|\\mathbf{y}, \\mathbf{x}, x^*)$, i.e. the **marginal distribution** of $y^*$ conditioned on the observed data, i.e. $p(y^*|\\mathbf{y}, \\mathbf{x}, x^*)$ which can be obtained by marginalizing over $\\alpha$ and $\\beta$ via the **sum rule**:\n",
    "\n",
    "$\\begin{align}\n",
    "p(y^*|\\mathbf{y}, \\mathbf{x}, x^*) = \\int\\int p(y^*, \\alpha, \\beta|\\mathbf{y}, \\mathbf{x}, x^*) \\text{d} \\alpha \\text{d}\\beta= \\int\\int \\underbrace{p(y^*|x^*, \\alpha, \\beta)}_{\\text{likelihood for }y^*} \\underbrace{p(\\alpha, \\beta|\\bm{y}, \\bm{x})}_{\\text{posterior distribution}} \\text{d} \\alpha \\text{d}\\beta,\n",
    "\\end{align}\n",
    "$\n",
    "as we derived in the lecture. The last equation shows that we can obtain the posterior predictive distribution by averaging the predictive likelihood wrt. to the posterior distribution. Hence, our first effort will be to get a handle on the posterior distribution of $\\alpha$ and $\\beta$ conditioned on $\\mathbf{y}$.\n",
    "\n",
    "Before starting the practical implementation, we will practice the terminology and the probabilistic machinery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1**: Use the **product rule** to write down the joint distribution of $\\mathbf{y}, \\alpha, $ and $\\beta$ given $\\mathbf{x}$:\n",
    "\n",
    "*Hint*: This is meant to be an easy task.\n",
    "\n",
    "\n",
    "\n",
    "**Task 1.2**: Show that $p(\\mathbf{y}, \\alpha, \\beta|\\mathbf{x}) = \\sum_{y^*}  p(\\mathbf{y}, y^*, \\alpha, \\beta|\\mathbf{x}, x^*)$, where the sum is over all possible outcomes of $y^*$.\n",
    "\n",
    "\n",
    "\n",
    "**Task 1.3**: Use Bayes' rule to express the posterior $p(\\alpha, \\beta|\\mathbf{y}, \\mathbf{x})$ in terms of the prior, likelihood and evidence and argue that the posterior is proportional to the joint distribution $p(\\mathbf{y}, \\alpha, \\beta|\\mathbf{x})$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:  The prior, likelihood and posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to implement code for evaluating the prior, likelihood and posterior distribution. Due to a combination of **numerical precision** and **mathematical convenience**, we will implement functions for evaluating the logarithm of these distributions. \n",
    "Below you are given an incomplete implementation of the logistic regression model. Your task is to complete the implementation. The code contains a few sanity checks that will help to detect potential bugs and errors in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 2.1**: Complete the implementation of the functions *f, theta, log_prior* and *log_likelihood* below.\n",
    "\n",
    "*Hints*:\n",
    "- *For implementing the log prior, the function log_npdf is handy.*\n",
    "- *When implementing the log likelihood, you can  either use *scipy.stats.binom* (imported as binom_dist) or you can implement the PMF of the binomial distribution from scratch.*\n",
    "- *Note that for each function the output must have the same shape as the inputs. That is, if you run log_prior(1, 1), it should return a scalar. If you run log_prior(alpha_vec, beta_vec), where alpha_vec and beta_vec are [10 x 1] arrays, then the results should also be a [10 x 1] array*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1./(1 + jnp.exp(-x))\n",
    "log_npdf = lambda x, m, v: -(x-m)**2/(2*v) - 0.5*jnp.log(2*jnp.pi*v)\n",
    "\n",
    "class LogisticRegression(object):\n",
    "\n",
    "    def __init__(self, x, y, N, sigma2_alpha=1., sigma2_beta=1.):\n",
    "        # data\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.N = N\n",
    "\n",
    "        # hyperparameters\n",
    "        self.sigma2_alpha = sigma2_alpha\n",
    "        self.sigma2_beta = sigma2_beta\n",
    "\n",
    "    def f(self, x, alpha, beta):\n",
    "        \"\"\" implements eq. (3). Output must have the same shape as x \"\"\"\n",
    "        return <insert code here>\n",
    "        \n",
    "    def theta(self, x, alpha, beta):\n",
    "        \"\"\" implements eq. (2). Output must have the same shape as x \"\"\"\n",
    "        return <insert code here>\n",
    "\n",
    "    def log_prior(self, alpha, beta):\n",
    "        \"\"\" implements log. of eq. (8). Output must have the same shape as alpha and beta \"\"\"\n",
    "        return <insert code here>\n",
    "\n",
    "    def log_likelihood(self, alpha, beta):\n",
    "        \"\"\" implements log. of eq. (5). Output must have the same shape as alpha and beta \"\"\"\n",
    "        theta = <insert code here>\n",
    "        loglik = <insert code here>\n",
    "\n",
    "        return log_lik\n",
    "\n",
    "    def log_joint(self, alpha, beta):\n",
    "        return self.log_prior(alpha, beta).squeeze() + self.log_likelihood(alpha, beta).squeeze()\n",
    "    \n",
    "# instantiate model\n",
    "model = LogisticRegression(x, y, N)\n",
    "\n",
    "# some sanity checks to help verify your implementation and make it compatible with the rest of the exercise\n",
    "assert jnp.allclose(model.theta(x=2, alpha=2, beta=-0.5), 0.7310585786300049), \"The value of output of the theta-function was different than expected. Please check your code.\"\n",
    "assert jnp.allclose(model.log_prior(1., 1.), -2.8378770664093453), \"The value of the output of the log_prior function was different than expected. Please check your code.\"\n",
    "assert jnp.allclose(model.log_likelihood(-1.,2.), -95.18926297085957), \"The value of the output of the log_likelihood function was different than expected. Please check your code.\"\n",
    "\n",
    "assert model.theta(jnp.linspace(-3, 3, 10), 1, 1).shape == (10,), \"The shape of the output of the theta-function was different than expected. Please check your code.\"\n",
    "assert model.log_prior(jnp.linspace(-3, 3, 10), jnp.linspace(-3, 3, 10)).shape == (10,), \"The shape of the output of the log_prior-function was different than expected. Please check your code.\"\n",
    "assert model.log_likelihood(jnp.linspace(-3, 3, 10)[:, None], jnp.linspace(-3, 3, 10)[:, None]).shape == (10, 1),  \"The shape of the output of the log_likelihood-function was different than expected. Please check your code.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model implementation above is correct, then the code below should produce contour plots of the prior, likelihood and posterior. We also compute and plot the MLE and MAP estimators for the model. Because this is a 2D problem, we can simply obtain the MLE/MAP estimators by maximizing over the set of grid values. However, this becomes infeasible in higher dimensions, where gradient-based optimization is used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid2D(object):\n",
    "    \"\"\" helper class for evaluating the function func on the grid defined by (alpha, beta)\"\"\"\n",
    "\n",
    "    def __init__(self, alphas, betas, func, name=\"Grid2D\"):\n",
    "        self.alphas = alphas\n",
    "        self.betas = betas\n",
    "        self.grid_size = (len(self.alphas), len(self.betas))\n",
    "        self.alpha_grid, self.beta_grid = jnp.meshgrid(alphas, betas, indexing='ij')\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "        \n",
    "        # evaluate function on each grid point\n",
    "        self.values = self.func(self.alpha_grid[:, :, None], self.beta_grid[:, :, None]).squeeze()\n",
    "\n",
    "    def plot_contours(self, ax, color='b', num_contours=10, f=lambda x: x, alpha=1.0, title=None):\n",
    "        ax.contour(self.alphas, self.betas, f(self.values).T, num_contours, colors=color, alpha=alpha)\n",
    "        ax.set(xlabel='$\\\\alpha$', ylabel='$\\\\beta$')\n",
    "        ax.set_title(self.name, fontweight='bold')\n",
    "\n",
    "    @property\n",
    "    def argmax(self):\n",
    "        idx = jnp.argmax(self.values)\n",
    "        alpha_idx, beta_idx = jnp.unravel_index(idx, self.grid_size)\n",
    "        return self.alphas[alpha_idx], self.betas[beta_idx]\n",
    "\n",
    "\n",
    "# define grid points (the only reason for using different numbers of points for alpha and beta is to make debugging easier)\n",
    "num_alpha, num_beta = 90, 100\n",
    "alphas = jnp.linspace(-4, 4, num_alpha)\n",
    "betas = jnp.linspace(-4, 4, num_beta)\n",
    "\n",
    "# evalute log prior, log likelihood and log joint on the (alpha, beta)-grid\n",
    "model = LogisticRegression(x, y, N, sigma2_alpha=1.0, sigma2_beta=1.0)\n",
    "log_prior_grid = Grid2D(alphas, betas, model.log_prior, 'Prior')\n",
    "log_lik_grid = Grid2D(alphas, betas, model.log_likelihood, 'Likelihood')\n",
    "log_joint_grid = Grid2D(alphas, betas, model.log_joint, 'Posterior')\n",
    "\n",
    "# visualize\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "log_prior_grid.plot_contours(ax[2], f=jnp.exp, color='b', alpha=0.2)\n",
    "log_lik_grid.plot_contours(ax[2], f=jnp.exp, color='r', alpha=0.2)\n",
    "\n",
    "for idx_plot, grid in enumerate([log_prior_grid, log_lik_grid, log_joint_grid]):\n",
    "    grid.plot_contours(ax[idx_plot], f=jnp.exp, color=colors[idx_plot])\n",
    "\n",
    "# compute and plot MLE and MAP estimators\n",
    "alpha_MLE, beta_MLE = log_lik_grid.argmax\n",
    "alpha_MAP, beta_MAP = log_joint_grid.argmax\n",
    "\n",
    "ax[2].plot(alpha_MLE, beta_MLE, 'rx', label='MLE')\n",
    "ax[2].plot(alpha_MAP, beta_MAP, 'gx', label='MAP')\n",
    "ax[2].legend();\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2**: When plotting the contours of the posterior distribution, we actually evaluate the joint distribution and not the posterior distribution . Why is this okay when making contour plots?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3**: Use the code above to experiment and investigate the relationship between the prior, likelihood and posterior. What happens if you change the prior variances of $\\alpha, \\beta$? E.g. $\\sigma^2_{\\alpha} = \\sigma^2_{\\beta} = 10$, $\\sigma^2_{\\alpha} = \\sigma^2_{\\beta} = 100$, $\\sigma^2_{\\alpha} = \\sigma^2_{\\beta} = 0.1$? What happens if $\\sigma^2_\\alpha = 1$ and $\\sigma^2_{\\beta} = 10$?\n",
    "\n",
    "**Note**: *This is a discussion question, which means that you actively have to experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.4**: What happens to the MLE and MAP estimators when you change the variances of the prior?\n",
    "\n",
    "**Note**: *This is a discussion question, which means that you actively have to experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5**: What happens to the prior, likelihood and posterior if you only use a subset of the data? Use the code above to experiment and investigate what happens, e.g. use the first 10 data instead of the full $M = 23$ datapoints.\n",
    "\n",
    "**Note**: *This is a discussion question, which means that you actively have to experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.6**: Use the plots to roughly estimate the prior and posterior probability of the event $\\beta < 0 $.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3:  Making predictions using point estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model for the probability of failure for each individual o-ring is given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\theta(x) = \\sigma(\\alpha + \\beta x),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\alpha, \\beta \\in \\mathbb{R}$ are unknown parameters. Since we do not know the values of the parameters, we can **estimate** them from the data in several ways. The simplest way is via the maximum likelihood estimator (MLE). That is, picking the **point**  that maximizes the likelihood of the data:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{\\alpha}_{\\text{MLE}}, \\hat{\\beta}_{\\text{MLE}} = \\arg\\max_{\\alpha, \\beta \\in \\mathbb{R}} p(\\mathbf{y}|\\mathbf{x}, \\alpha, \\beta).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In practice, we most often due this using gradient-based optimization. However, in this exercise, we will simply pick the parameters values that maximizes the likelihood function on the grid, which works well for 2D problems. Instead of using the MLE, we can also use the Maximum a posteriori (MAP) estimator. That is, picking the **point** that maximizes the posterior distribution:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{\\alpha}_{\\text{MAP}}, \\hat{\\beta}_{\\text{MAP}} = \\arg\\max_{\\alpha, \\beta} p(\\alpha, \\beta|\\mathbf{y},\\mathbf{x}).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After obtaining an estimate of the parameters, we can use a **plugin approximation** for $\\theta(x)$ by simply inserting the **point estimate** (e.g. MLE or MAP) for $\\alpha$ and $\\beta$:\n",
    "$$\\begin{align*}\n",
    "\\hat{\\theta}(x) = \\sigma(\\hat{\\alpha} + \\hat{\\beta} x),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and then use $\\hat{\\theta}$ for making predictions.\n",
    "\n",
    "\n",
    "**Task 3.1**: Compute and plot the predicted probability $\\hat{\\theta}(x)$ for $x$ corresponding to temperatures $t \\in \\left[-10, 40\\right]$ for both MLE and MAP estimators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for predictions\n",
    "temp_pred = jnp.linspace(-10, 40, 500)\n",
    "x_pred = standardize(temp_pred)\n",
    "N_pred = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4:  Approximating the posterior using a grid approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using point estimates, we will now turn to full Bayesian inference to be able to properly reason above the uncertainty of both parameters and predictions. In contrast to the Beta-binomial model, the logistic regression model is **not a conjugate model**, which means we cannot compute the posterior distribution $p(\\alpha, \\beta|\\mathbf{y}, \\mathbf{x})$ analytically. Instead, we will use a very simple **grid approximation** $q(\\alpha, \\beta)$ to represent the posterior distribution:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\alpha, \\beta|\\mathbf{y}, \\mathbf{x}) \\approx q(\\alpha, \\beta).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To construct the grid approximation $q(\\alpha, \\beta)$, we need three steps:\n",
    "\n",
    "1) **Define grid**: First we need to the define grid for $(\\alpha, \\beta)$. Typically, we choose an appropriate discrete set of values for $\\alpha$ and $\\beta$ individually, i.e. $\\left\\lbrace \\alpha_i \\right\\rbrace_{i=1}^I$ and $\\left\\lbrace \\beta_i \\right\\rbrace_{j=1}^J$, and then take the $(\\alpha, \\beta)$-grid to be the Cartesian product.\n",
    "\n",
    "2) **Evaluation:** Next, we need to evaluate the function $p(\\alpha, \\beta| \\mathbf{y, x})$ on each point in the $(\\alpha, \\beta)$-grid. Importantly, here we only need to evaluate the posterior **up to a normalization constant**, and hence, **we can evaluate the joint distribution instead of the posterior**. Moreover, for numerical reasons, we often evaluate the **logarithm of the joint distribution**.\n",
    "\n",
    "3) **Normalization**: To ensure that our approximation is a valid distribution, we need to make it sum to $1$. To achieve this, we compute the sum of $p(\\mathbf{y}, \\alpha, \\beta| \\mathbf{x})$ across all grid points to approximate the normalization constant $Z$.\n",
    "\n",
    "That is, \n",
    "$$\\begin{align*}\n",
    "q(\\alpha_i, \\beta_j) = \\frac{1}{Z}p(\\bm{y}, \\alpha_i, \\beta_j| \\mathbf{x}) = \\frac{1}{Z}\\tilde{\\pi}_{ij} = \\pi_{ij}, \n",
    "\\end{align*}\n",
    "$$\n",
    "where we have defined $\\log \\tilde{\\pi}_{ij} = \\log p(\\bm{y}, \\alpha_i, \\beta_j| \\mathbf{x})$ and $\\pi_{ij} = \\frac{1}{Z}\\tilde{\\pi}_{ij}$ for each point on the grid $(\\alpha_i, \\beta_j)$. The normalization constant then becomes\n",
    "$$\\begin{align*}\n",
    "Z = \\sum_{i,j} p(\\mathbf{y}, \\alpha_i, \\beta_j| \\mathbf{ x}) = \\sum_{i,j}\\tilde{\\pi}_{ij} = \\sum_{i=1}^I \\sum_{j=1}^J \\tilde{\\pi}_{ij},\n",
    "\\end{align*}\n",
    "$$\n",
    "where the sum is over all the points in the grid. Note that $q(\\alpha, \\beta) = 0$ when evaluated on any point not belonging to the pre-defined $(\\alpha, \\beta)$-grid.\n",
    "\n",
    "As we will see later, the grid approximation makes it easy for us to\n",
    "\n",
    "1) compute expectations with respect to the posterior\n",
    "\n",
    "2) generate samples from the posterior, which will allow us to make inferences about the parameters $\\alpha$ and $\\beta$ as well as make predictions will taking the uncertainty into account.\n",
    "\n",
    "The purpose of the class $\\texttt{GridApproximation2D}$ given below is to approximate the posterior distribution of the logistic regression model using a grid approximation. We will re-use the code from the $\\texttt{Grid2D}$-class to implemented step 1) and step 2) above. Hence, we basically also need to carry out the step 3.\n",
    "\n",
    "**Task 4.1**: Complete the implementation of the `prep approximation`-function below. Your task is to make sure that the `probability grid`-variable contains all the $\\pi_{ij}$ defined above with the proper normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridApproximation2D(Grid2D):\n",
    "\n",
    "    def __init__(self, alphas, betas, log_joint, threshold=1e-8, name=\"GridApproximation2D\"):\n",
    "        Grid2D.__init__(self, alphas, betas, log_joint, name)\n",
    "        self.threshold = threshold\n",
    "        self.prep_approximation()\n",
    "        self.compute_marginals()\n",
    "        self.sanity_check()\n",
    "        \n",
    "    def prep_approximation(self):\n",
    "        \n",
    "        # [num_alpha, num_beta]-sized matrix of the log joint evaluated on the grid \n",
    "        self.log_joint_grid = self.values\n",
    "        self.log_joint_grid = self.log_joint_grid - jnp.max(self.log_joint_grid)\n",
    "\n",
    "\n",
    "\n",
    "        # [num_alpha, num_beta]-matrix of \\pi_{ij}-values summing to 1.\n",
    "        self.probabilities_grid = <insert code here>\n",
    "\n",
    "        # flatten for later convinience\n",
    "        self.alphas_flat = self.alpha_grid.flatten()                                             # shape: [num_alpha*num_beta] = [num_outcomes]\n",
    "        self.betas_flat = self.beta_grid.flatten()                                               # shape: [num_alpha*num_beta] = [num_outcomes]\n",
    "        self.num_outcomes = len(self.alphas_flat)                                                # shape: scalar \n",
    "        self.probabilities_flat = self.probabilities_grid.flatten()                              # [num_outcomes]\n",
    "\n",
    "    def compute_marginals(self):\n",
    "        self.pi_alpha =  0 #<replace 0 with your solution>\n",
    "        self.pi_beta =  0 #<replace 0 with your solution>\n",
    "\n",
    "        # compute marginal distribution using sum rule\n",
    "    def compute_expectation(self, f):\n",
    "        \"\"\" computes expectation of f(alpha, beta) wrt. the grid approximation \"\"\"\n",
    "        return jnp.sum(f(self.alphas_flat, self.betas_flat)*self.probabilities_flat, axis=0)\n",
    "    \n",
    "    def sample(self, key, num_samples=1):\n",
    "        \"\"\" generate num_samples from the grid approximation distribution \"\"\"\n",
    "        idx = random.choice(key, jnp.arange(self.num_outcomes), p=self.probabilities_flat, shape=(num_samples, 1))\n",
    "        return self.alphas_flat[idx], self.betas_flat[idx]\n",
    "\n",
    "    def visualize(self, ax, scaling=8000, title='Grid approximation'):\n",
    "        idx = self.probabilities_flat > self.threshold\n",
    "        ax.scatter(self.alphas_flat[idx], self.betas_flat[idx], scaling*self.probabilities_flat[idx],label='$\\\\pi_{ij}$')        \n",
    "        ax.set(xlabel='$\\\\alpha$', ylabel='$\\\\beta$')\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "\n",
    "    def sanity_check(self):\n",
    "        assert self.probabilities_grid.shape == self.grid_size, \"Probability grid does not have shape [num_alphas, num_betas] (self.grid_size). Check your implementation.\"\n",
    "        assert jnp.all(self.probabilities_grid >= 0), \"Not all values in probability grid are non-negative. Check your implementation.\"\n",
    "        assert jnp.allclose(self.probabilities_grid.sum(), 1), \"Values in probability grid do not sum to one. Check your implementation.\"\n",
    "\n",
    "# grid\n",
    "num_alpha, num_beta = 90, 100\n",
    "alphas = jnp.linspace(-4, 4, num_alpha)\n",
    "betas = jnp.linspace(-4, 4, num_beta)\n",
    "\n",
    "# model\n",
    "model = LogisticRegression(x, y, N, sigma2_alpha=1.0, sigma2_beta=1.0)\n",
    "\n",
    "# construct grid approximation for posterior\n",
    "post_approx = GridApproximation2D(alphas, betas, model.log_joint, name='Posterior')\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "post_approx.visualize(ax, title='Grid approximation of posterior distribution $p(\\\\alpha, \\\\beta|  \\\\mathbf{y}, \\\\mathbf{x})$')\n",
    "ax.plot(alpha_MLE, beta_MLE, 'r.', label='MLE', markersize=10);\n",
    "ax.plot(alpha_MAP, beta_MAP, 'g.', label='MAP', markersize=10);\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous task was implemented correctly, the code about should produce a visualization of the grid approximation. The blue dots show the position of all the grid points, and the radius of the blue dots is controlled by the size of the corresponding weight $\\pi_{ij}$. For easier interpretation, we do not plot points where $\\pi_{ij}$ is smaller than $10^{-8}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our first order of business after having obtained the posterior distribution is to produce estimates of the parameters $\\alpha$ and $\\beta$ as well to quantify the uncertainty, i.e. we will estimate the posterior mean, posterior standard deviation and 95% credibility interval for both $\\alpha$ and $\\beta$. For this purpose, it is more convenient to work with the marginal posterior distributions of $\\alpha$ and $\\beta$, which can be readily obtained from the joint posterior approximation $q(\\alpha, \\beta)$ using the **sum rule**:\n",
    "\n",
    "$$\\begin{align*}\n",
    "q(\\alpha_i) &= \\sum_{j} q(\\alpha_i, \\beta_j) = \\sum_{j} \\pi_{ij} = \\pi^{\\alpha}_i\\\\\n",
    "q(\\beta_j) &= \\sum_{i} q(\\alpha_i, \\beta_j) = \\sum_{i} \\pi_{ij} = \\pi^{\\beta}_j,\n",
    "\\end{align*}$$\n",
    "\n",
    "i.e. $\\pi^{\\alpha} = q(\\alpha_i)$ is the (approximate) marginal posterior probability of $p(\\alpha = \\alpha_i|\\mathbf{y}, \\mathbf{x})$.\n",
    "\n",
    "**Task 4.2**: Go back up the class $\\texttt{GridApproximation2D}$ and implemented the $\\texttt{compute marginals}$-function. Use it to compute and plot the probability mass functions for the approximate posterior marginal distributions $q(\\alpha)$ and $q(\\beta)$ as a function of $\\alpha$ and $\\beta$, respectively.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.3**: Compute the posterior mean and standard deviation for $\\alpha$ and $\\beta$ using the grid approximation by completing the implementation of `DiscreteDistribution1D`-class below. Use the figures from the previous task to verify that your results are (approximately) correct.\n",
    "\n",
    "*Hints*:\n",
    "- The mean of a discrete random variable X with PMF $P(X = x)$ is given by $\\mathbb{E}\\left[X\\right] = \\sum_{i} x_i P(X = x_i)$, where the sum is over all the possible outcomes of $X$.\n",
    "- The variance of a discrete random variable X with PMF $P(X = x)$ is given by $\\mathbb{V}\\left[X\\right] = \\sum_{i} \\left(x_i - \\mathbb{E}\\left[X\\right]\\right)^2 P(X = x_i)$, where the sum is over all the possible outcomes of $X$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiscreteDistribution1D(object):\n",
    "\n",
    "    def __init__(self, outcomes, probabilities, name='DiscreteDistribution'):\n",
    "        \"\"\" represents discrete random variable X in terms of outcomes and probabilities \"\"\"\n",
    "        self.outcomes = outcomes\n",
    "        self.probabilities = probabilities\n",
    "        assert self.outcomes.shape == self.probabilities.shape\n",
    "        self.name = name\n",
    "\n",
    "    def CDF(self, x):\n",
    "        \"\"\" P[X <= x] \"\"\"\n",
    "        idx = self.outcomes <= x\n",
    "        return jnp.sum(self.probabilities[idx]) \n",
    "    \n",
    "    def quantile(self, p):\n",
    "        \"\"\" Q(p) = inf {x | p < CDF(x)} \"\"\"\n",
    "        cdf_values = jnp.cumsum(self.probabilities) \n",
    "        idx = jnp.where(jnp.logical_or(p < cdf_values, jnp.isclose(p, cdf_values)))[0]\n",
    "        return jnp.min(self.outcomes[idx])\n",
    "    \n",
    "    @property\n",
    "    def mean(self):\n",
    "        \"\"\" return scalar corresponding to the mean of the discrete distribution \"\"\"\n",
    "        return <insert code here>\n",
    "    \n",
    "    @property\n",
    "    def variance(self):\n",
    "        \"\"\" return scalar corresponding to the variance of the discrete distribution \"\"\"\n",
    "        return <insert code here>\n",
    "    \n",
    "    def central_interval(self, interval_size=95):\n",
    "        \"\"\" return tuple (lower, upper) corresponding to the central interval of the discrete distribution \"\"\"\n",
    "        c = 1.-interval_size/100.\n",
    "        lower = self.quantile(c/2)\n",
    "        upper = self.quantile(1-c/2)\n",
    "        return jnp.array([lower, upper])  \n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(f'Summary for {self.name}')\n",
    "        print(f'\\tMean:\\t\\t\\t\\t{self.mean:3.2f}')\n",
    "        print(f'\\tStd. dev.:\\t\\t\\t{jnp.sqrt(self.variance):3.2f}')\n",
    "        print(f'\\t95%-credibility interval:\\t[{self.central_interval()[0]:3.2f}, {self.central_interval()[1]:3.2f}]\\n')\n",
    "\n",
    "\n",
    "# example use and sanity sanity checks using classic six-sided die\n",
    "outcomes, probs = jnp.arange(1,6+1), jnp.ones(6)/6\n",
    "sixsided_die = DiscreteDistribution1D(outcomes, probs)\n",
    "assert jnp.allclose(sixsided_die.mean, 3.5), f\"The mean of a sixsided die should be 3.5, but the value was {sixsided_die.mean:3.2f}. Check your implementation.\"\n",
    "assert jnp.allclose(sixsided_die.variance, 35/12), f\"The variance of a sixsided die should be approximately 2.92, but the value was {sixsided_die.variance:3.2f}. Check your implementation.\"\n",
    "assert jnp.allclose(sixsided_die.central_interval(95), jnp.array([1, 6])), f\"The (approximate) 95% central interval of a sixsided die should be (1,6), but the value was {sixsided_die.central_interval(95)}. Check your implementation.\"\n",
    "\n",
    "# summarize posterior marginals for alpha and beta\n",
    "post_alpha_marginal = DiscreteDistribution1D(post_approx.alphas, post_approx.pi_alpha, \"posterior distribution of alpha\")\n",
    "post_beta_marginal = DiscreteDistribution1D(post_approx.betas, post_approx.pi_beta, \"posterior distribution of beta\")\n",
    "\n",
    "post_alpha_marginal.print_summary()\n",
    "post_beta_marginal.print_summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the summary statistics to the plots above. Verify that your results are consistent before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Task 4.4**: Compute the prior and posterior probability that $\\beta < 0$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5:  Propagating uncertainty from parameters to predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the grid approximation, we got a handle on the posterior distribution of $\\alpha$ and $\\beta$ given the observed data, i.e. $p(\\alpha, \\beta|\\mathbf{x}, \\mathbf{y})$. We will now turn our attention to making predictions. For example, we want to predict the following quantities for a given temperature $t^* \\in \\mathbb{R}$:\n",
    "\n",
    "- The probability of failure for each o-ring\n",
    "- The probability of at least one o-ring failing\n",
    "- The expected number of failing o-rings\n",
    "- A 95\\%-credibility interval for then number of failed o-rings\n",
    "\n",
    "**Assuming we know** the value of $\\alpha$ and $\\beta$, all quantities above are easily computed from the predictive likelihood\n",
    "\n",
    "$$\\begin{align}\n",
    "p(y^*|x^*, \\alpha, \\beta) = \\text{Bin}(y^*|N^*, \\alpha, \\beta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "However, in practice we do not now the value of $\\alpha$ and $\\beta$, and therefore, we need to estimate them. The simplest approach for making predictions is to use a **plugin approximation** like we did in part 3. That is, we simply plug in one of our point estimates for $\\alpha$ and $\\beta$ into the equation above, e.g. the MLE or MAP estimator. This is usually fast and easy to do, but the significant drawback is that it tends to produce **overconfident** predictions because it ignores the uncertainty for $\\theta^*$. Instead, we can propagate the uncertainty from the posterior distribution $q(\\alpha, \\beta)$ to the predictive distribution using **marginalization** via the sum:\n",
    "\n",
    "$\\begin{align}\n",
    "p(y^*|\\mathbf{y}, \\mathbf{x}, x^*) = \\int \\int p(y^*|x^*, \\alpha, \\beta) p(\\alpha, \\beta|\\mathbf{y}, \\mathbf{x}) \\text{d}\\alpha \\text{d}\\beta = \\mathbb{E}_{p(\\alpha, \\beta|\\mathbf{y}, \\mathbf{x})}\\left[p(y^*|x^*, \\alpha, \\beta)\\right] \\approx \\mathbb{E}_{q(\\alpha, \\beta)}\\left[p(y^*|x^*, \\alpha, \\beta)\\right].\n",
    "\\end{align}$\n",
    "\n",
    "That is, when we make predictions, we average over the posterior distribution. **Instead of picking a single set of parameters, we consider all possible values for $\\alpha$ and $\\beta$ and weigh them according to their posterior probability**. This may be abstract, but the grid approximation can help understand this more intuitively:\n",
    "\n",
    "$\\begin{align}\n",
    "p(y^*|\\mathbf{y}, \\mathbf{x}, x^*) = \\mathbb{E}_{p(\\alpha, \\beta|\\mathbf{y})}\\left[p(y^*|x^*, \\alpha, \\beta)\\right]\\approx \\mathbb{E}_{q(\\alpha, \\beta)}\\left[p(y^*|x^*, \\alpha, \\beta)\\right] = \\sum_{ij} p(y^*|x^*, \\alpha_i, \\beta_j) \\pi_{ij},\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where we literally evaluate $p(y^*|x^*, \\alpha, \\beta)$ for each possible set of parameters $(\\alpha_i, \\beta_j)$ and weigh them by the posterior probability $\\pi_{ij} = q(\\alpha_i, \\beta_j)$. We refer to the distribution $p(y^*|\\mathbf{y}, \\mathbf{x}, x^*)$ as the **posterior predictive distribution**. Recall, we can also cast the plugin approximations as approximation over the posterior predictive distribution, where the posterior is approximated by a posterior distribution that as all the mass on the single point $(\\hat{\\alpha}_{MLE}, \\hat{\\beta}_{\\text{MLE}})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses the plugin approximation for the MLE/MAP estimator to compute and plot the posterior predictive distribution $p(y^*=k|\\mathbf{y}, \\mathbf{x}, x^*)$ for $k \\in \\left\\lbrace 0, 1, \\dots 6\\right\\rbrace$. The code also reports the mean and a 95\\% interval for each methods for each temperatures (assuming the tasks above have been solved). \n",
    "\n",
    "**Task 5.1**: Compute and plot and the full posterior predictive distribution $p(y^*=k|\\mathbf{y}, \\mathbf{x}, x^*)$ with $k \\in \\left\\lbrace 0, 1, \\dots 6\\right\\rbrace$ for $t = 25^{\\circ}C$ and $t = 0^{\\circ}C$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target temperatures t^*\n",
    "temp_stars = [25, 0]\n",
    "\n",
    "# outcome space for y^*\n",
    "ks = jnp.arange(0, 6+1)\n",
    "\n",
    "# predictive likelihood p(y^*=k|x^*, alpha, beta)\n",
    "pred_lik = lambda k, x_star, alpha, beta: binom_dist.pmf(k=k, n=6, p=model.theta(x_star, alpha, beta))\n",
    "\n",
    "# prep figure\n",
    "fig, ax = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "mean_and_interval_label = lambda dist: f'Mean: {dist.mean:3.2f}, 95%-interval: [{dist.central_interval()[0]:d}, {dist.central_interval()[1]:d}]'\n",
    "\n",
    "for n, t_star in enumerate(temp_stars):\n",
    "\n",
    "    # standardize to get x^*\n",
    "    x_star = standardize(t_star)\n",
    "\n",
    "    # compute plugin approximations using MLE and MAP. Shape must be [7] (matching the shape of ks)\n",
    "    pred_lik_probs_MLE = pred_lik(ks, x_star, alpha_MLE, beta_MLE)\n",
    "    pred_lik_probs_MAP = pred_lik(ks, x_star, alpha_MAP, beta_MAP)\n",
    "    \n",
    "    # compute posterior predictive distribution as a weighted averaged over all parameter vales. Shape must be [7] (matching the shape of ks)\n",
    "    pred_lik_bayes_probs = np.zeros(7) <replace with your code here>\n",
    "\n",
    "    # prepare for plotting etc\n",
    "    pred_dist_MLE = DiscreteDistribution1D(ks, pred_lik_probs_MLE, name='MLE Plug-in approximation')\n",
    "    pred_dist_MAP = DiscreteDistribution1D(ks, pred_lik_probs_MAP, name='MAP Plug-in approximation')\n",
    "    pred_dist_bayes = DiscreteDistribution1D(ks, pred_lik_bayes_probs, name=\"Posterior predictive distribution\")\n",
    "\n",
    "    # plot\n",
    "    for i, dist in enumerate([pred_dist_MLE, pred_dist_MAP, pred_dist_bayes]):\n",
    "        ax[n, i].bar(ks, dist.probabilities, label=mean_and_interval_label(dist))\n",
    "        ax[n, i].set(xlabel='k', ylabel='$p(y^* = k|\\\\mathbf{y})$', ylim =(0, 1))\n",
    "        ax[n, i].set_ylim((0, 1))\n",
    "        ax[n, i].set_title(f'{dist.name} for $t^* = {t_star}^oC$', fontweight='bold')\n",
    "        ax[n, i].set(xlabel='k', ylabel='$p(y^* = k|\\\\mathbf{y})$', ylim =(0, 1))\n",
    "        ax[n, i].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2**: Comment on the differences between the results obtained using plugin approximations, i.e. the MLE/MAP estimators, and the posterior predictive distributions. How does the three approximations compare? In regions with data? In regions without data?\n",
    "\n",
    "**Note**: *This is a discussion question, which means that you actively have to experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the very last of the exercise, we will visualize the posterior for $f(x)$ and $\\theta(x)$ to build over intuition. We will start by drawing a set of random samples of $(\\alpha, \\beta)$ from the posterior approximation and then compute and visualize $f(x)$ and $\\theta(x)$ for each sample. We will repeat this process for the prior distribution $p(\\alpha, \\beta)$.\n",
    "\n",
    "**Task 5.3**: The code below generates $\\texttt{num samples}$ of $\\alpha, \\beta$ from both the prior and posterior and plots them. Compute $f(x_{\\text{pred}})$ and $\\theta(x_{\\text{pred}})$ for each of these samples and store them in the variables $\\texttt{f samples}$ and $\\texttt{theta samples}$ pre-defined below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for predictions\n",
    "temp_pred = jnp.linspace(-10, 40, 500)\n",
    "x_pred = standardize(temp_pred)\n",
    "N_pred = 6\n",
    "\n",
    "# number of samples plot\n",
    "num_samples = 100\n",
    "\n",
    "# grid\n",
    "num_alpha, num_beta = 90, 100\n",
    "alphas = jnp.linspace(-4, 4, num_alpha)\n",
    "betas = jnp.linspace(-4, 4, num_beta)\n",
    "\n",
    "# set up prior and posterior for visualization purposes\n",
    "model = LogisticRegression(x, y, N, sigma2_alpha=1., sigma2_beta=1.)\n",
    "post_approx = GridApproximation2D(alphas, betas, model.log_joint, name='Posterior')\n",
    "prior_approx = GridApproximation2D(alphas, betas, model.log_prior, name='Prior')\n",
    "\n",
    "# specify seed\n",
    "seed = 123\n",
    "key = random.PRNGKey(seed)\n",
    "\n",
    "\n",
    "# plot for each distribution...\n",
    "fig, ax = plt.subplots(2, 3, figsize=(20, 8))\n",
    "for idx_plot, dist in enumerate([prior_approx, post_approx]):\n",
    "    key, subkey = random.split(key)\n",
    "\n",
    "    # plot data and contours of distribution\n",
    "    plot_data(ax[idx_plot, 2], counts=False)\n",
    "    dist.plot_contours(ax[idx_plot, 0], f=jnp.exp, color=colors[idx_plot])\n",
    "\n",
    "    # get (alpha, beta)-samples to visualize\n",
    "    alpha_samples, beta_samples = dist.sample(subkey, num_samples)\n",
    "\n",
    "    # compute samples of f(x_pred) and theta(x_pred). Shape must be [num_samples, len(temp_pred)] for both f_samples and theta_samples\n",
    "    f_samples = jnp.zeros((num_samples, len(temp_pred))) <replace with your code here>\n",
    "     theta_samples = jnp.zeros((num_samples, len(temp_pred))) <replace with your code here>\n",
    "\n",
    "    # plot\n",
    "    ax[idx_plot, 0].plot(alpha_samples, beta_samples, '.', color=colors[idx_plot])\n",
    "\n",
    "    # plot each sample of f and theta\n",
    "    ax[idx_plot, 1].plot(temp_pred, f_samples.T, '', color=colors[idx_plot], alpha=0.1)\n",
    "    ax[idx_plot, 2].plot(temp_pred, theta_samples.T, '', color=colors[idx_plot], alpha=0.1)\n",
    "    ax[idx_plot, 2].legend()\n",
    "\n",
    "    # title and labels\n",
    "    ax[idx_plot, 0].set(title=f'{dist.name} samples of $(\\\\alpha, \\\\beta)$')\n",
    "    ax[idx_plot, 1].set(title=f'{dist.name} samples of $f(x) = \\\\alpha + \\\\beta x$', xlabel='Temperature')\n",
    "    ax[idx_plot, 2].set(title=f'{dist.name} sample of $\\\\theta(x) = \\\\sigma(\\\\alpha + \\\\beta x)$')\n",
    "\n",
    "fig.suptitle('Prior and posterior predictions', fontweight='bold')\n",
    "fig.subplots_adjust(hspace=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.4**: Study the code and plots above and make sure you can explain the following:\n",
    "\n",
    "- What is plotted in each of the six panels? \n",
    "- What is the relationship between each of the dots in the left columns and the curves in the center and right columns?\n",
    "- How the uncertainty on $f(x)$ and $\\theta(x)$ is controlled by the distribution of $(\\alpha, \\beta)$?\n",
    "- How well does each of the prior samples match the observed data?\n",
    "- How well does each of the posterior samples match the observed data?\n",
    "\n",
    "**Note**: *This is a discussion question, which means that you actively have to experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use posterior samples to the summarize the pointwise posterior mean and 95\\% intervals of $f(x)$ and $\\theta(x)$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 0\n",
    "key = random.PRNGKey(seed)\n",
    "\n",
    "# set up prior and posterior for visualization purposes\n",
    "model = LogisticRegression(x, y, N, sigma2_alpha=1., sigma2_beta=1.)\n",
    "post_approx = GridApproximation2D(alphas, betas, model.log_joint, name='Posterior')\n",
    "prior_approx = GridApproximation2D(alphas, betas, model.log_prior, name='Prior')\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "plot_data(ax[0], counts=False)\n",
    "plot_data(ax[1], counts=False)\n",
    "\n",
    "num_samples = 2000\n",
    "\n",
    "for idx_plot, dist in enumerate([prior_approx, post_approx]):\n",
    "    key, subkey = random.split(key)\n",
    "\n",
    "    alpha_samples, beta_samples = dist.sample(subkey, num_samples)\n",
    "\n",
    "    theta_samples = model.theta(x_pred, alpha_samples, beta_samples)\n",
    "    theta_mean = theta_samples.mean(0)\n",
    "    theta_lower = jnp.quantile(theta_samples, 0.025, axis=0)\n",
    "    theta_upper = jnp.quantile(theta_samples, 1-0.025,)\n",
    "\n",
    "    ax[idx_plot].plot(temp_pred, theta_mean, 'g-', label='Mean')\n",
    "    for interval_size, b in [(0.95, 0.1), (0.8, 0.2), (0.5, 0.4)]:\n",
    "        c = 1-interval_size\n",
    "        theta_lower = jnp.quantile(theta_samples, c/2, axis=0)\n",
    "        theta_upper = jnp.quantile(theta_samples, 1-c/2, axis=0)\n",
    "        ax[idx_plot].fill_between(temp_pred, theta_lower, theta_upper, color='g', alpha=b, label=f'{interval_size*100:2.0f}%-interval')\n",
    "    ax[idx_plot].legend()\n",
    "    ax[idx_plot].set_title(f'{dist.name} predictive probability of o-ring failure', fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.5**: Compute and plot the posterior mean and a 95% credibility interval for the probability of obtaining at least one failure among the 6 o-rings\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATEL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
